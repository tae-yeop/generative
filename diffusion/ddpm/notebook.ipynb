{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf0fe6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from functools import partial\n",
    "from inspect import isfunction\n",
    "from functools import partial\n",
    "from torch import einsum\n",
    "from einops import rearrange, reduce\n",
    "from einops.layers.torch import Rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e53355f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        *, \n",
    "        in_channels, \n",
    "        out_channels=None, \n",
    "        time_emb_dim=None, \n",
    "        groups=8, \n",
    "        eps=1e-6, \n",
    "        dropout = 0.0,\n",
    "        time_embedding_norm=\"scale_shift\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.time_embedding_norm = time_embedding_norm\n",
    "\n",
    "        self.norm1 = nn.GroupNorm(num_groups=groups, num_channels=in_channels, eps=eps, affine=True)\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=1)\n",
    "\n",
    "        if time_emb_dim is not None:\n",
    "            if self.time_embedding_norm == \"default\":\n",
    "                self.time_emb_proj = nn.Sequential(\n",
    "                    nn.SiLU(),\n",
    "                    nn.Linear(time_emb_dim, out_channels)\n",
    "                )\n",
    "            elif self.time_embedding_norm == \"scale_shift\":\n",
    "                self.time_emb_proj = nn.Sequential(\n",
    "                    nn.SiLU(),\n",
    "                    nn.Linear(time_emb_dim, out_channels * 2)\n",
    "                )\n",
    "                \n",
    "        else:\n",
    "            self.time_emb_proj = None\n",
    "\n",
    "        self.norm2 = nn.GroupNorm(num_groups=groups, num_channels=out_channels, eps=eps, affine=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, stride=1)\n",
    "        self.act = nn.SiLU()\n",
    "            \n",
    "        self.conv_shortcut = None\n",
    "        if self.use_shortcut:\n",
    "            self.conv_shortcut = nn.Conv2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size=1,\n",
    "                padding=0,\n",
    "                stride=1,\n",
    "                bias=True\n",
    "            )\n",
    "\n",
    "    def forward(self, x, temb=None):\n",
    "        \"\"\"\n",
    "        x : [B, C, H, W]\n",
    "        temb : [B, time_emb_dim]\n",
    "        \"\"\"\n",
    "        hidden_states = x\n",
    "\n",
    "        hidden_states = self.norm1(hidden_states)\n",
    "        hidden_states = self.act(hidden_states)\n",
    "        hidden_states = self.conv1(hidden_states)\n",
    "\n",
    "        if self.time_emb_proj is not None:\n",
    "            temb = self.time_emb_proj(temb)[:,:,None,None]\n",
    "\n",
    "        if self.time_embedding_norm == \"default\":\n",
    "            if temb is not None:\n",
    "                hidden_states = hidden_states + temb\n",
    "            hidden_states = self.norm2(hidden_states)\n",
    "        elif self.time_embedding_norm == \"scale_shift\":\n",
    "            time_scale, time_shift = torch.chunk(temb, 2, dim=1)\n",
    "            hidden_states = (1 + time_scale) * hidden_states + time_shift\n",
    "        else:\n",
    "            hidden_states = self.norm2(hidden_states)\n",
    "\n",
    "        hidden_states = self.act(hidden_states)\n",
    "\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.conv2(hidden_states)\n",
    "\n",
    "        if self.conv_shortcut is not None:\n",
    "            x = self.conv_shortcut(x.contiguous())\n",
    "\n",
    "        output = (x + hidden_states)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574daac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownSample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels=None, down_sample=\"full\"):\n",
    "        super().__init__()\n",
    "\n",
    "        if out_channels is None:\n",
    "            out_channels = in_channels\n",
    "\n",
    "        if down_sample == 'full':\n",
    "            # No More Strided Convolutions or Pooling\n",
    "            self.conv = nn.Sequential(\n",
    "                Rearrange(\"b c (h p1) (w p2) -> b (c p1 p2) h w\", p1=2, p2=2),\n",
    "                nn.Conv2d(in_channels * 4, out_channels, kernel_size=1)\n",
    "            )\n",
    "        elif dowm_sampe == 'padding':\n",
    "            self.conv = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n",
    "        else:\n",
    "            self.conv = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "    \n",
    "    def forward(self, x, output_size=None):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UpSample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels=None, interpolate=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.interpolate = interpolate\n",
    "\n",
    "        if interpolate:\n",
    "            self.conv = nn.Conv2d(in_channels, out_channels, 3, padding=1, bias=True)\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                nn.Upsample(scale_factor=2, mode=\"nearest\"),\n",
    "                nn.Conv2d(in_channels, out_channels, 3, padding=1, bias=True)\n",
    "            )\n",
    "\n",
    "    def forward(self, x, output_size=None):\n",
    "        if self.interpolate:\n",
    "            if output_size is None:\n",
    "                x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n",
    "            else:\n",
    "                x = F.interpolate(x, size=output_size, mode=\"nearest\")\n",
    "\n",
    "            out = self.conv(x)\n",
    "        else:\n",
    "            out = self.conv(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25582c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "i = torch.randn(4, 5, 6)\n",
    "x,y,z = i.shape\n",
    "print(type(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfe9e885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8, 8, 5],\n",
       "        [2, 3, 8],\n",
       "        [6, 1, 5]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randint(0, 10, (3, 3))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f449dd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8],\n",
       "        [8],\n",
       "        [6]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.amax(dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337971d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, in_channels, n_heads, dim_head):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        hidden_dim = dim_head * n_heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.qkv = nn.Conv2d(in_channels, hidden_dim * 3, 1, bias=False)\n",
    "        self.to_out = nn.Conv2d(hidden_dim, in_channels, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        qkv = self.qkv(x).chunk(3, dim=1)\n",
    "        q, k, v = map(\n",
    "            lambda t: rearrange(t, \"b (h c) x y -> b h c (x y)\", h=self.n_heads), qkv\n",
    "        )\n",
    "\n",
    "        q = q * self.scale\n",
    "\n",
    "        sim = einsum(\"b h d i, b h d j -> b h i j\", q , k)\n",
    "        sim = sim - sim.amax(dim=-1, keepdim=True).detach()\n",
    "        attn = sim.softmax(dim=-1)\n",
    "\n",
    "        out = einsum(\"b h i j, b h d j -> b h i d\", attn, v)\n",
    "        out = rearrange(out, \"b h (x y) d -> b (h d) x y\", x=h, y=w)\n",
    "        return self.to_out(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d304b0e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0279, 0.0484, 0.7742, 0.0836, 0.0659],\n",
       "        [0.3463, 0.4633, 0.0170, 0.1600, 0.0134]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(10)\n",
    "a.view(2, 5).softmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "535c91f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGroupNorm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mnum_groups\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mnum_channels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0meps\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-05\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0maffine\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m Initialize internal Module state, shared by both nn.Module and ScriptModule.\n",
      "\u001b[0;31mFile:\u001b[0m      /opt/conda/lib/python3.10/site-packages/torch/nn/modules/normalization.py\n",
      "\u001b[0;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "nn.GroupNorm.__init__?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcc6a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearSelfAttention(nn.Module):\n",
    "    def __init__(self, in_channels, n_heads, dim_head):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        hidden_dim = dim_head * n_heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "        self.to_qkv = nn.Conv2d(in_channels, hidden_dim * 3, 1, bias=False)\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Conv2d(hidden_dim, in_channels, 1),\n",
    "            nn.GroupNorm(1, in_channels)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=1)\n",
    "        q, k, v = map(\n",
    "            lambda t: rearrange(t, \"b (h c) x y -) b h c (x y)\", h=self.n_heads), qkv\n",
    "        )\n",
    "\n",
    "        q = q.softmax(dim=-2)\n",
    "        k = k.softmax(dim=-1)\n",
    "\n",
    "        q = q * self.scale\n",
    "        context = torch.einsum(\"b h d n, b h e n -> b h d e\", k, v)\n",
    "\n",
    "        out = torch.einsum(context, q)\n",
    "        out = rearrange(out, \"b h c (x y) -> b (h c) x y\", h=self.n_heads, x=h, y=w)\n",
    "        out = self.to_out(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8f1ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttnDownBlocak(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers,\n",
    "        in_channels, \n",
    "        time_emb_dim,\n",
    "        out_channels=None, \n",
    "        down_sample=\"full\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        for i in range(num_layers):\n",
    "            resnets.append(\n",
    "                ResBlock(\n",
    "                    in_channels=in_channels,\n",
    "                    out_channels=out_channels,\n",
    "                    time_emb_dim=\n",
    "\n",
    "                )\n",
    "            )\n",
    "            attentions.append(\n",
    "                SelfAttention2D(\n",
    "                    out_channels,\n",
    "                    heads=4,\n",
    "                    dim_head=32\n",
    "                )\n",
    "\n",
    "            )\n",
    "\n",
    "        self.resnets = nn.ModuleList(resnets)\n",
    "        self.attentions = nn.ModuleList(attentions)\n",
    "        self.downsample = Downsample(out_channels, out_channels, down_sample)\n",
    "\n",
    "    def forward(self, x, temb):\n",
    "        hidden_states = x\n",
    "        for resnet, attn in zip(self.resnets, self.attentions):\n",
    "            hidden_states = resnet(hidden_states, temb)\n",
    "            hidden_states = attn(hidden_states)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
