{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Discrete Representation Learning, Aaron van den Oord et al., \n",
    "\n",
    "### 관련 자료\n",
    "[유튜브 강의](https://www.youtube.com/watch?v=WTqPCPeipEY)\n",
    "\n",
    "코드 🧑‍💻\n",
    "- https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/autoencoders/vae.py\n",
    "- https://github.com/rosinality/vq-vae-2-pytorch/blob/master/vqvae.py\n",
    "- https://keras.io/examples/generative/vq_vae/\n",
    "https://github.com/nadavbh12/VQ-VAE\n",
    "https://github.com/rosinality/vq-vae-2-pytorch/blob/master/vqvae.py\n",
    "https://github.com/lucidrains/vector-quantize-pytorch/blob/master/vector_quantize_pytorch/vector_quantize_pytorch.py\n",
    "https://github.com/ritheshkumar95/pytorch-vqvae\n",
    "https://nbviewer.org/github/zalandoresearch/pytorch-vq-vae/blob/master/vq-vae.ipynb\n",
    "https://github.com/HwangJohn/vq-vae-test/blob/main/model/vqvae.py\n",
    "\n",
    "### 핵심 정리\n",
    "1. VAE에 continuous latent space(normal distribution)가 아닌 discrete latent embedding(categorical distribution)을 적용하였다\n",
    "2. 즉 딕셔너리 형태로 discrete 카테고리 매핑할 수 있다\n",
    "3. 이름은 Variational이 들어가지만 그냥 AE이다. (KLD가 없다)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivations\n",
    "\n",
    "[1] Discrete categorical nature\n",
    "- 실 세계의 물체들을 discrete category로 생각할 수 있다 : cat, car, etc.\n",
    "- DNA sequence : A,C,G,T\n",
    "- 알파벳 : A,B,C …\n",
    "\n",
    "\n",
    "[2] Posterior collapse\n",
    "\n",
    "- Poseteior collapse 정의\n",
    "    - $x$에서 posterior parameter로 전달되는 시그널이 너무 약하거나 노이즈가 많으면 posterior이 collapsing된다고 표현함\n",
    "    - 이 경우 posterior $q_{\\phi}(z |x)$에서 나오는 샘플 $z$ ($z \\sim q_{\\phi}(z|x)$)가 디코더에서 무시되기 시작함\n",
    "    - 시그널이 noisy하다는 것은  $\\mu_d, \\sigma_d$가 unstable ⇒ 샘플된 $z$ 또한 unstable ⇒ 디코더가 $z$를 무시하기 시작\n",
    "    - Decoder가 z를 무시하게 된다 ⇒ $\\hat{x}$가 $z$와 independent해진다.\n",
    "\n",
    "- Posterior collapse 원인\n",
    "    - 1) 인코더와 디코더의 불균형\n",
    "    - 인코더가 너무 약하거나 ⇒ 의미있는 시그널을 충분히 인코딩하지 않게 됨\n",
    "    - 인코더가 너무 강하거나 ⇒ 데이터안에 있는 노이즈까지 인코딩\n",
    "    - 디코더가 상대적으로 강해하면 ⇒  의미없는 정보를 알아서 맞춰서 시그널을 살려냄\n",
    "\n",
    "    - 2) ELBO와 evidence 사이의 gap, true posterior approximation의 실패\n",
    "    - 3) Ill-posed problem이기 때문에 조건에 맞는 다양한 latent z가 존재할 수 있는 가능성\n",
    "    - 4) 가정한 Gaussian prior가 적합하지 않는 가능성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image](https://github.com/user-attachments/assets/1d4acc4e-12ed-403c-8298-436b017ca8a4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3가지 구성\n",
    "- `Encoder`\n",
    "- `Decoder` : 코드북 벡터를 이용해 디코딩\n",
    "- `VQ Layer`\n",
    "    - `embedding dictionary (코드북)` $e \\in \\Bbb{R}^{K \\times D}$를 포함하도록 함\n",
    "    - 각각의 임베딩 하나는 $e_i \\in \\Bbb{R}^D, i \\in [1, \\dots, K]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Encoder에서 나온 `z_e`와 `코드북`의 각각 임베딩과 비교를 해서 가장 비슷한 녀석에 대응되는 값으로 치환하도록 함\n",
    "\n",
    "- 인덱스만 따져보면 Poseterior categorial distribution 형태로 생각할 수 있음\n",
    "$$\n",
    "q(z=k |x) = \\begin{cases}\n",
    "   1 &\\text{for}  k = \\argmin_j ||z_e(x) - e_j ||_2 \\\\\n",
    "   0 &\\text{otherwise }\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "![Image](https://github.com/user-attachments/assets/355773d1-2f9c-4032-b338-b269d7f8aa4f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_embeddings,\n",
    "        embedding_dim,\n",
    "        commitment_cost,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.commitment_cost = commitment_cost\n",
    "\n",
    "        # 코드북 생성\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        # 초기화\n",
    "        nn.init.uniform_(self.embedding.weight, -1.0/self.num_embeddings, 1.0/self.num_embeddings)\n",
    "\n",
    "    def forward(self, z_e):\n",
    "        \"\"\"\n",
    "        z_e: (B, D, H, W)\n",
    "        return:\n",
    "          z_q: quantized output same shape (B, D, H, W)\n",
    "          vq_loss, commit_loss\n",
    "          encoding_indices: (B, H, W)\n",
    "        \"\"\"\n",
    "        # (B, D, H, W) -> (B*H*W, D)\n",
    "        z = z_e.permute(0, 2, 3, 1).contiguous()\n",
    "        z_flattened  = z.view(-1, self.embedding_dim)  # (B*H*W, D)\n",
    "\n",
    "        # (2) 임베딩 벡터와 거리 계산 후 (3) 가장 가까운 임베딩 index 선택\n",
    "        # torch.cdist 결과 형태: (B*H*W, K)\n",
    "        min_encoding_indices = torch.argmin(torch.cdist(z_flattened, self.embedding.weight), dim=1)\n",
    "\n",
    "        # (4) 인덱스로부터 임베딩 벡터를 가져온 뒤 (5) reshape\n",
    "        z_q = self.embedding(min_encoding_indices).view(z.shape) # (B*H*W, D)\n",
    "\n",
    "\n",
    "        z_q = z_q.view(z_e.size(0), z_e.size(2), z_e.size(3), self.embedding_dim) # (B,H,W,D)\n",
    "\n",
    "        z_q = z_q.permute(0,3,1,2).contiguous() # (B,D,H,W)\n",
    "\n",
    "        loss = torch.zeros((z.shape[0])).to(z.device, dtype=z.dtype)\n",
    "        if self.training:\n",
    "            # (6) 임베딩 파라미터 쪽 gradient 계산\n",
    "            vq_loss = self.commitment_cost * torch.mean((z_q.detach() - z_e) ** 2)\n",
    "            # (7) 인코더 파라미터 쪽 gradient 계산\n",
    "            commit_loss = torch.mean((z_q - z_e.detach()) ** 2)\n",
    "\n",
    "            loss = vq_loss + self.commitment_cost * commit_loss\n",
    "\n",
    "        # quantized output: z_e + (z_q - z_e).detach()\n",
    "        # trick: pass z_q gradient to encoder\n",
    "        z_q_out = z_e + (z_q - z_e).detach()\n",
    "\n",
    "        return z_q_out, loss, min_encoding_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objectives\n",
    "\n",
    "![Image](https://github.com/user-attachments/assets/59f31927-cb95-4c07-b20f-fe9d08021bb5)\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\mathcal{L}_{cookbook} + \\mathcal{L}_{recon} + \\mathcal{L}_{commitment} \\\\\n",
    "\\mathcal{L}_{recon} = ||D(E(x)) - x||^2_2 \\\\\n",
    "\\mathcal{L}_{cookbook} = \\frac{1}{s} \\sum_s ||sg[h_s] - e_{z_z}||^2_2\n",
    "\n",
    "$$\n",
    "- Recon loss : encoder, decoder를 최적화\n",
    "- VQ Loss(Codebook Loss)\n",
    "    - 임베딩을 업데이트하는 역할 \n",
    "    - 코드북이 인코더 아웃풋에 있을 법한 것들로 구성되게끔\n",
    "    - gradient = ||sg[z_e] - e||^2\n",
    "- Commitment Loss\n",
    "    - 인코더 파라미터 업데이트하는 역할\n",
    "    - 인코더가 되도록이면 코드북 스러운 아웃풋을 내도록\n",
    "    - gradient = ||z_e - sg[e]||^2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위처럼 이중으로 Loss를 거는 이유는 VQ Layer안의 argmin operation 때문\n",
    "- non-linear + 미분 불가능 (not differentiated with respect to its input) ⇒ backprop이 안됨\n",
    "\n",
    "<img src=\"https://github.com/user-attachments/assets/51c66354-bcc6-44ef-8f3f-63bbfd2e7660\" width=\"250\" height=\"250\"/>\n",
    "\n",
    "\n",
    "- 디코더 쪽에서 오는 grad는 VQ를 무시하고 인코더에 그대로 복사하는 식으로 전달 가능 (straight-through estimator)\n",
    "- encoder와 decoder가 같은 channel space를 갖기 때문에 decoder의 gradient가 여전히 encoder에도 도움이 되길 기대\n",
    "- 디코더 쪽의 grad는 VQ Layer로 전달되지 않는 상황\n",
    "\n",
    "- VQ Loss와 Commitment Loss를 분리하지 않고 ||z_e - z_q|| 하면 안되는 이유 : 서로 고정된 참조 대상으로 삼아야 학습이 안정 (마치 DQN에서 off-policy하듯이)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Swish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)\n",
    "    \n",
    "\n",
    "class Upsample(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            channels: int,\n",
    "            use_conv: bool = False,\n",
    "            use_conv_transpose: bool = False,\n",
    "            out_channels = None,\n",
    "            interpolate=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.out_channels = out_channels or channels\n",
    "        self.use_conv = use_conv\n",
    "        self.use_conv_transpose = use_conv_transpose\n",
    "        self.interpolate = interpolate\n",
    "\n",
    "        conv = None\n",
    "        if use_conv_transpose:\n",
    "            conv = nn.ConvTranspose2d(\n",
    "                channels, self.out_channels, kernel_size=4, stride=2, padding=1, bias=True\n",
    "            )\n",
    "        elif use_conv:\n",
    "            conv = nn.Conv2d(self.channels, self.out_channels, kernel_size=3, padding=1, bias=True)\n",
    "\n",
    "        self.conv = conv\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        if self.use_conv_transpose:\n",
    "            return self.conv(hidden_states)\n",
    "        \n",
    "        dtype = hidden_states.dtype\n",
    "        if dtype == torch.bfloat16 and is_torch_version(\"<\", \"2.1\"):\n",
    "            hidden_states = hidden_states.to(torch.float32)\n",
    "\n",
    "        hidden_states = F.interpolate(hidden_states, scale_factor=2.0, mode=\"nearest\")\n",
    "\n",
    "        if dtype == torch.bfloat16 and is_torch_version(\"<\", \"2.1\"):\n",
    "            hidden_states = hidden_states.to(dtype)\n",
    "\n",
    "        if self.use_conv:\n",
    "            hidden_states = self.conv(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class Downsample(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            channels: int,\n",
    "            use_conv: bool = False,\n",
    "            out_channels: Optional[int] = None,\n",
    "            padding: int = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.out_channels = out_channels or channels\n",
    "        self.use_conv = use_conv\n",
    "        self.padding = padding\n",
    "\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        if self.use_conv and self.padding == 0:\n",
    "            pad = (0, 1, 0, 1)\n",
    "            hidden_states = F.pad(hidden_states, pad, mode=\"constant\", value=0)\n",
    "\n",
    "        hidden_states = self.conv(hidden_states)\n",
    "        return hidden_states\n",
    "    \n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            groups,\n",
    "            eps,\n",
    "            use_in_shortcut: Optional[bool] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.norm1 = torch.nn.GroupNorm(num_groups=groups, num_channels=in_channels, eps=eps, affine=True)\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.norm2 = torch.nn.GroupNorm(num_groups=groups_out, num_channels=out_channels, eps=eps, affine=True)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.nonlinearity = Swish()\n",
    "\n",
    "\n",
    "        if self.up:\n",
    "\n",
    "\n",
    "        self.use_in_shortcut = self.in_channels != out_channels if use_in_shortcut is None else use_in_shortcut\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        GroupNorm > Swish > Conv > UpSample DownSample > GroupNorm > Swish > Conv > Shortcut\n",
    "        \"\"\"\n",
    "        hidden_states = x\n",
    "        hidden_states = self.norm1(hidden_states)\n",
    "        hidden_states = self.nonlinearity(hidden_states)\n",
    "\n",
    "        if self.upsample is not None:\n",
    "        elif self.downsample is not None:\n",
    "\n",
    "        hidden_states = self.conv1(hidden_states)\n",
    "\n",
    "        hidden_states = self.norm2(hidden_states)\n",
    "        hidden_states = self.nonlinearity(hidden_states)\n",
    "\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.conv2(hidden_states)\n",
    "\n",
    "        if self.conv_shortcut is not None:\n",
    "            x = self.conv_shortcut(x)\n",
    "\n",
    "        output_tensor = (x + hidden_states)\n",
    "        return output_tensor\n",
    "\n",
    "\n",
    "class DownEncoderBlock(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        \n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            block_out_channels: Tuple[int, ...] = (64,),\n",
    "\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv_in = nn.Conv2d(\n",
    "            in_channels,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "        )\n",
    "\n",
    "        # down\n",
    "        self.down_blocks = nn.ModuleList([])\n",
    "\n",
    "\n",
    "        # out\n",
    "        self.conv_norm_out = nn.GroupNorm(num_channels=block_out_channels[-1], num_groups=norm_num_groups, eps=1e-6)\n",
    "        self.conv_act = nn.SiLU()\n",
    "        self.conv_out = nn.Conv2d(block_out_channels[-1], out_channels, 3, padding=1)\n",
    "\n",
    "\n",
    "    def forward(self, sample):\n",
    "        sample = self.conv_in(sample)\n",
    "        # down\n",
    "        for down_block in self.down_blocks:\n",
    "            sample = down_block(sample)\n",
    "\n",
    "        # post-process\n",
    "        sample = self.conv_norm_out(sample)\n",
    "        sample = self.conv_act(sample)\n",
    "        sample = self.conv_out(sample)\n",
    "\n",
    "        return sample\n",
    "    \n",
    "\n",
    "class Decoder(nn.Module):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQVAE(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_channels=3,\n",
    "        embedding_dim=64,\n",
    "        num_embeddings=512,\n",
    "        commitment_cost=0.25,\n",
    "        hidden_channels=128,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "        \n",
    "        self.encoder = Encoder()\n",
    "        self.quant_conv = nn.Conv2d(latent_channels, vq_embed_dim, 1)\n",
    "        self.vq = VectorQuantizer()\n",
    "        self.post_quant_conv = nn.Conv2d(vq_embed_dim, latent_channels, 1)\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        h = self.quant_conv(h)\n",
    "        return h\n",
    "        \n",
    "    def decode(self, h):\n",
    "        z_q, vq_loss, _, perplexity = self.vq(h)\n",
    "        z_q2 = self.post_quant_conv(z_q)\n",
    "        dec = self.decoder(z_q2)\n",
    "        return dec, vq_loss, perplexity\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encode(x)\n",
    "        x_recon, vq_loss, perplexity = self.decode(h)\n",
    "        return x_recon, vq_loss, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "\n",
    "batch_size = 256\n",
    "num_training_updates = 10000\n",
    "num_hiddens = 128\n",
    "num_residual_hiddens = 32\n",
    "num_residual_layers = 2\n",
    "embedding_dim = 64\n",
    "num_embeddings = 512\n",
    "commitment_cost = 0.25\n",
    "decay = 0.99\n",
    "learning_rate = 1e-3\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "training_data = datasets.CIFAR10(root=\"/home/aiteam/tykim/generative_model/data\", train=True, download=True,\n",
    "                                  transform=transforms.Compose([\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize((0.5,0.5,0.5), (1.0,1.0,1.0))\n",
    "                                  ]))\n",
    "\n",
    "validation_data = datasets.CIFAR10(root=\"/home/aiteam/tykim/generative_model/data\", train=False, download=True,\n",
    "                                  transform=transforms.Compose([\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize((0.5,0.5,0.5), (1.0,1.0,1.0))\n",
    "                                  ]))\n",
    "\n",
    "\n",
    "training_loader = DataLoader(training_data, \n",
    "                             batch_size=batch_size, \n",
    "                             shuffle=True,\n",
    "                             pin_memory=True)\n",
    "\n",
    "validation_loader = DataLoader(validation_data,\n",
    "                               batch_size=32,\n",
    "                               shuffle=True,\n",
    "                               pin_memory=True)\n",
    "\n",
    "\n",
    "model = VQVAE(embedding_dim=embedding_dim, num_embeddings=num_embeddings, commitment_cost=commitment_cost).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, amsgrad=False)\n",
    "\n",
    "\n",
    "model.train()\n",
    "train_res_recon_error = []\n",
    "train_res_perplexity = []\n",
    "\n",
    "for i in range(num_training_updates):\n",
    "    (data, _) = next(iter(training_loader))\n",
    "    data = data.to(device)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    data_recon, vq_loss, perplexity = model(data)\n",
    "    recon_error = F.mse_loss(data_recon, data, reduction='mean') * data.shape[0]\n",
    "    loss =  recon_error + vq_loss * data.shape[0]\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    train_res_recon_error.append(recon_error.item())\n",
    "    train_res_perplexity.append(perplexity.item())\n",
    "\n",
    "    if (i+1) % 100 == 0:\n",
    "        print('%d iterations' % (i+1))\n",
    "        print('recon_error: %.3f' % np.mean(train_res_recon_error[-100:]))\n",
    "        print('perplexity: %.3f' % np.mean(train_res_perplexity[-100:]))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "(valid_originals, _) = next(iter(validation_loader))\n",
    "valid_originals = valid_originals.to(device)\n",
    "\n",
    "vq_output_eval = model._pre_vq_conv(model._encoder(valid_originals))\n",
    "_, valid_quantize, _, _ = model._vq_vae(vq_output_eval)\n",
    "valid_reconstructions = model._decoder(valid_quantize)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
