{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Discrete Representation Learning, Aaron van den Oord et al., \n",
    "\n",
    "### ê´€ë ¨ ìë£Œ\n",
    "[ìœ íŠœë¸Œ ê°•ì˜](https://www.youtube.com/watch?v=WTqPCPeipEY)\n",
    "\n",
    "ì½”ë“œ ğŸ§‘â€ğŸ’»\n",
    "- https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/autoencoders/vae.py\n",
    "- https://github.com/rosinality/vq-vae-2-pytorch/blob/master/vqvae.py\n",
    "- https://keras.io/examples/generative/vq_vae/\n",
    "https://github.com/nadavbh12/VQ-VAE\n",
    "https://github.com/rosinality/vq-vae-2-pytorch/blob/master/vqvae.py\n",
    "https://github.com/lucidrains/vector-quantize-pytorch/blob/master/vector_quantize_pytorch/vector_quantize_pytorch.py\n",
    "https://github.com/ritheshkumar95/pytorch-vqvae\n",
    "https://nbviewer.org/github/zalandoresearch/pytorch-vq-vae/blob/master/vq-vae.ipynb\n",
    "https://github.com/HwangJohn/vq-vae-test/blob/main/model/vqvae.py\n",
    "\n",
    "### í•µì‹¬ ì •ë¦¬\n",
    "1. VAEì— continuous latent space(normal distribution)ê°€ ì•„ë‹Œ discrete latent embedding(categorical distribution)ì„ ì ìš©í•˜ì˜€ë‹¤\n",
    "2. ì¦‰ ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ discrete ì¹´í…Œê³ ë¦¬ ë§¤í•‘í•  ìˆ˜ ìˆë‹¤\n",
    "3. ì´ë¦„ì€ Variationalì´ ë“¤ì–´ê°€ì§€ë§Œ ê·¸ëƒ¥ AEì´ë‹¤. (KLDê°€ ì—†ë‹¤)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivations\n",
    "\n",
    "[1] Discrete categorical nature\n",
    "- ì‹¤ ì„¸ê³„ì˜ ë¬¼ì²´ë“¤ì„ discrete categoryë¡œ ìƒê°í•  ìˆ˜ ìˆë‹¤ : cat, car, etc.\n",
    "- DNA sequence : A,C,G,T\n",
    "- ì•ŒíŒŒë²³ : A,B,C â€¦\n",
    "\n",
    "\n",
    "[2] Posterior collapse\n",
    "\n",
    "- Poseteior collapse ì •ì˜\n",
    "    - $x$ì—ì„œ posterior parameterë¡œ ì „ë‹¬ë˜ëŠ” ì‹œê·¸ë„ì´ ë„ˆë¬´ ì•½í•˜ê±°ë‚˜ ë…¸ì´ì¦ˆê°€ ë§ìœ¼ë©´ posteriorì´ collapsingëœë‹¤ê³  í‘œí˜„í•¨\n",
    "    - ì´ ê²½ìš° posterior $q_{\\phi}(z |x)$ì—ì„œ ë‚˜ì˜¤ëŠ” ìƒ˜í”Œ $z$ ($z \\sim q_{\\phi}(z|x)$)ê°€ ë””ì½”ë”ì—ì„œ ë¬´ì‹œë˜ê¸° ì‹œì‘í•¨\n",
    "    - ì‹œê·¸ë„ì´ noisyí•˜ë‹¤ëŠ” ê²ƒì€  $\\mu_d, \\sigma_d$ê°€ unstable â‡’ ìƒ˜í”Œëœ $z$ ë˜í•œ unstable â‡’ ë””ì½”ë”ê°€ $z$ë¥¼ ë¬´ì‹œí•˜ê¸° ì‹œì‘\n",
    "    - Decoderê°€ zë¥¼ ë¬´ì‹œí•˜ê²Œ ëœë‹¤ â‡’ $\\hat{x}$ê°€ $z$ì™€ independentí•´ì§„ë‹¤.\n",
    "\n",
    "- Posterior collapse ì›ì¸\n",
    "    - 1) ì¸ì½”ë”ì™€ ë””ì½”ë”ì˜ ë¶ˆê· í˜•\n",
    "    - ì¸ì½”ë”ê°€ ë„ˆë¬´ ì•½í•˜ê±°ë‚˜ â‡’ ì˜ë¯¸ìˆëŠ” ì‹œê·¸ë„ì„ ì¶©ë¶„íˆ ì¸ì½”ë”©í•˜ì§€ ì•Šê²Œ ë¨\n",
    "    - ì¸ì½”ë”ê°€ ë„ˆë¬´ ê°•í•˜ê±°ë‚˜ â‡’ ë°ì´í„°ì•ˆì— ìˆëŠ” ë…¸ì´ì¦ˆê¹Œì§€ ì¸ì½”ë”©\n",
    "    - ë””ì½”ë”ê°€ ìƒëŒ€ì ìœ¼ë¡œ ê°•í•´í•˜ë©´ â‡’  ì˜ë¯¸ì—†ëŠ” ì •ë³´ë¥¼ ì•Œì•„ì„œ ë§ì¶°ì„œ ì‹œê·¸ë„ì„ ì‚´ë ¤ëƒ„\n",
    "\n",
    "    - 2) ELBOì™€ evidence ì‚¬ì´ì˜ gap, true posterior approximationì˜ ì‹¤íŒ¨\n",
    "    - 3) Ill-posed problemì´ê¸° ë•Œë¬¸ì— ì¡°ê±´ì— ë§ëŠ” ë‹¤ì–‘í•œ latent zê°€ ì¡´ì¬í•  ìˆ˜ ìˆëŠ” ê°€ëŠ¥ì„±\n",
    "    - 4) ê°€ì •í•œ Gaussian priorê°€ ì í•©í•˜ì§€ ì•ŠëŠ” ê°€ëŠ¥ì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image](https://github.com/user-attachments/assets/1d4acc4e-12ed-403c-8298-436b017ca8a4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3ê°€ì§€ êµ¬ì„±\n",
    "- `Encoder`\n",
    "- `Decoder` : ì½”ë“œë¶ ë²¡í„°ë¥¼ ì´ìš©í•´ ë””ì½”ë”©\n",
    "- `VQ Layer`\n",
    "    - `embedding dictionary (ì½”ë“œë¶)` $e \\in \\Bbb{R}^{K \\times D}$ë¥¼ í¬í•¨í•˜ë„ë¡ í•¨\n",
    "    - ê°ê°ì˜ ì„ë² ë”© í•˜ë‚˜ëŠ” $e_i \\in \\Bbb{R}^D, i \\in [1, \\dots, K]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Encoderì—ì„œ ë‚˜ì˜¨ `z_e`ì™€ `ì½”ë“œë¶`ì˜ ê°ê° ì„ë² ë”©ê³¼ ë¹„êµë¥¼ í•´ì„œ ê°€ì¥ ë¹„ìŠ·í•œ ë…€ì„ì— ëŒ€ì‘ë˜ëŠ” ê°’ìœ¼ë¡œ ì¹˜í™˜í•˜ë„ë¡ í•¨\n",
    "\n",
    "- ì¸ë±ìŠ¤ë§Œ ë”°ì ¸ë³´ë©´ Poseterior categorial distribution í˜•íƒœë¡œ ìƒê°í•  ìˆ˜ ìˆìŒ\n",
    "$$\n",
    "q(z=k |x) = \\begin{cases}\n",
    "   1 &\\text{for}  k = \\argmin_j ||z_e(x) - e_j ||_2 \\\\\n",
    "   0 &\\text{otherwise }\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "![Image](https://github.com/user-attachments/assets/355773d1-2f9c-4032-b338-b269d7f8aa4f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_embeddings,\n",
    "        embedding_dim,\n",
    "        commitment_cost,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.commitment_cost = commitment_cost\n",
    "\n",
    "        # ì½”ë“œë¶ ìƒì„±\n",
    "        self.embedding = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        # ì´ˆê¸°í™”\n",
    "        nn.init.uniform_(self.embedding.weight, -1.0/self.num_embeddings, 1.0/self.num_embeddings)\n",
    "\n",
    "    def forward(self, z_e):\n",
    "        \"\"\"\n",
    "        z_e: (B, D, H, W)\n",
    "        return:\n",
    "          z_q: quantized output same shape (B, D, H, W)\n",
    "          vq_loss, commit_loss\n",
    "          encoding_indices: (B, H, W)\n",
    "        \"\"\"\n",
    "        # (B, D, H, W) -> (B*H*W, D)\n",
    "        z = z_e.permute(0, 2, 3, 1).contiguous()\n",
    "        z_flattened  = z.view(-1, self.embedding_dim)  # (B*H*W, D)\n",
    "\n",
    "        # (2) ì„ë² ë”© ë²¡í„°ì™€ ê±°ë¦¬ ê³„ì‚° í›„ (3) ê°€ì¥ ê°€ê¹Œìš´ ì„ë² ë”© index ì„ íƒ\n",
    "        # torch.cdist ê²°ê³¼ í˜•íƒœ: (B*H*W, K)\n",
    "        min_encoding_indices = torch.argmin(torch.cdist(z_flattened, self.embedding.weight), dim=1)\n",
    "\n",
    "        # (4) ì¸ë±ìŠ¤ë¡œë¶€í„° ì„ë² ë”© ë²¡í„°ë¥¼ ê°€ì ¸ì˜¨ ë’¤ (5) reshape\n",
    "        z_q = self.embedding(min_encoding_indices).view(z.shape) # (B*H*W, D)\n",
    "\n",
    "\n",
    "        z_q = z_q.view(z_e.size(0), z_e.size(2), z_e.size(3), self.embedding_dim) # (B,H,W,D)\n",
    "\n",
    "        z_q = z_q.permute(0,3,1,2).contiguous() # (B,D,H,W)\n",
    "\n",
    "        loss = torch.zeros((z.shape[0])).to(z.device, dtype=z.dtype)\n",
    "        if self.training:\n",
    "            # (6) ì„ë² ë”© íŒŒë¼ë¯¸í„° ìª½ gradient ê³„ì‚°\n",
    "            vq_loss = self.commitment_cost * torch.mean((z_q.detach() - z_e) ** 2)\n",
    "            # (7) ì¸ì½”ë” íŒŒë¼ë¯¸í„° ìª½ gradient ê³„ì‚°\n",
    "            commit_loss = torch.mean((z_q - z_e.detach()) ** 2)\n",
    "\n",
    "            loss = vq_loss + self.commitment_cost * commit_loss\n",
    "\n",
    "        # quantized output: z_e + (z_q - z_e).detach()\n",
    "        # trick: pass z_q gradient to encoder\n",
    "        z_q_out = z_e + (z_q - z_e).detach()\n",
    "\n",
    "        return z_q_out, loss, min_encoding_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objectives\n",
    "\n",
    "![Image](https://github.com/user-attachments/assets/59f31927-cb95-4c07-b20f-fe9d08021bb5)\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\mathcal{L}_{cookbook} + \\mathcal{L}_{recon} + \\mathcal{L}_{commitment} \\\\\n",
    "\\mathcal{L}_{recon} = ||D(E(x)) - x||^2_2 \\\\\n",
    "\\mathcal{L}_{cookbook} = \\frac{1}{s} \\sum_s ||sg[h_s] - e_{z_z}||^2_2\n",
    "\n",
    "$$\n",
    "- Recon loss : encoder, decoderë¥¼ ìµœì í™”\n",
    "- VQ Loss(Codebook Loss)\n",
    "    - ì„ë² ë”©ì„ ì—…ë°ì´íŠ¸í•˜ëŠ” ì—­í•  \n",
    "    - ì½”ë“œë¶ì´ ì¸ì½”ë” ì•„ì›ƒí’‹ì— ìˆì„ ë²•í•œ ê²ƒë“¤ë¡œ êµ¬ì„±ë˜ê²Œë”\n",
    "    - gradient = ||sg[z_e] - e||^2\n",
    "- Commitment Loss\n",
    "    - ì¸ì½”ë” íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸í•˜ëŠ” ì—­í• \n",
    "    - ì¸ì½”ë”ê°€ ë˜ë„ë¡ì´ë©´ ì½”ë“œë¶ ìŠ¤ëŸ¬ìš´ ì•„ì›ƒí’‹ì„ ë‚´ë„ë¡\n",
    "    - gradient = ||z_e - sg[e]||^2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ìœ„ì²˜ëŸ¼ ì´ì¤‘ìœ¼ë¡œ Lossë¥¼ ê±°ëŠ” ì´ìœ ëŠ” VQ Layerì•ˆì˜ argmin operation ë•Œë¬¸\n",
    "- non-linear + ë¯¸ë¶„ ë¶ˆê°€ëŠ¥ (not differentiated with respect to its input) â‡’ backpropì´ ì•ˆë¨\n",
    "\n",
    "<img src=\"https://github.com/user-attachments/assets/51c66354-bcc6-44ef-8f3f-63bbfd2e7660\" width=\"250\" height=\"250\"/>\n",
    "\n",
    "\n",
    "- ë””ì½”ë” ìª½ì—ì„œ ì˜¤ëŠ” gradëŠ” VQë¥¼ ë¬´ì‹œí•˜ê³  ì¸ì½”ë”ì— ê·¸ëŒ€ë¡œ ë³µì‚¬í•˜ëŠ” ì‹ìœ¼ë¡œ ì „ë‹¬ ê°€ëŠ¥ (straight-through estimator)\n",
    "- encoderì™€ decoderê°€ ê°™ì€ channel spaceë¥¼ ê°–ê¸° ë•Œë¬¸ì— decoderì˜ gradientê°€ ì—¬ì „íˆ encoderì—ë„ ë„ì›€ì´ ë˜ê¸¸ ê¸°ëŒ€\n",
    "- ë””ì½”ë” ìª½ì˜ gradëŠ” VQ Layerë¡œ ì „ë‹¬ë˜ì§€ ì•ŠëŠ” ìƒí™©\n",
    "\n",
    "- VQ Lossì™€ Commitment Lossë¥¼ ë¶„ë¦¬í•˜ì§€ ì•Šê³  ||z_e - z_q|| í•˜ë©´ ì•ˆë˜ëŠ” ì´ìœ  : ì„œë¡œ ê³ ì •ëœ ì°¸ì¡° ëŒ€ìƒìœ¼ë¡œ ì‚¼ì•„ì•¼ í•™ìŠµì´ ì•ˆì • (ë§ˆì¹˜ DQNì—ì„œ off-policyí•˜ë“¯ì´)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Swish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)\n",
    "    \n",
    "\n",
    "class Upsample(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            channels: int,\n",
    "            use_conv: bool = False,\n",
    "            use_conv_transpose: bool = False,\n",
    "            out_channels = None,\n",
    "            interpolate=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.out_channels = out_channels or channels\n",
    "        self.use_conv = use_conv\n",
    "        self.use_conv_transpose = use_conv_transpose\n",
    "        self.interpolate = interpolate\n",
    "\n",
    "        conv = None\n",
    "        if use_conv_transpose:\n",
    "            conv = nn.ConvTranspose2d(\n",
    "                channels, self.out_channels, kernel_size=4, stride=2, padding=1, bias=True\n",
    "            )\n",
    "        elif use_conv:\n",
    "            conv = nn.Conv2d(self.channels, self.out_channels, kernel_size=3, padding=1, bias=True)\n",
    "\n",
    "        self.conv = conv\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        if self.use_conv_transpose:\n",
    "            return self.conv(hidden_states)\n",
    "        \n",
    "        dtype = hidden_states.dtype\n",
    "        if dtype == torch.bfloat16 and is_torch_version(\"<\", \"2.1\"):\n",
    "            hidden_states = hidden_states.to(torch.float32)\n",
    "\n",
    "        hidden_states = F.interpolate(hidden_states, scale_factor=2.0, mode=\"nearest\")\n",
    "\n",
    "        if dtype == torch.bfloat16 and is_torch_version(\"<\", \"2.1\"):\n",
    "            hidden_states = hidden_states.to(dtype)\n",
    "\n",
    "        if self.use_conv:\n",
    "            hidden_states = self.conv(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class Downsample(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            channels: int,\n",
    "            use_conv: bool = False,\n",
    "            out_channels: Optional[int] = None,\n",
    "            padding: int = 1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.out_channels = out_channels or channels\n",
    "        self.use_conv = use_conv\n",
    "        self.padding = padding\n",
    "\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        if self.use_conv and self.padding == 0:\n",
    "            pad = (0, 1, 0, 1)\n",
    "            hidden_states = F.pad(hidden_states, pad, mode=\"constant\", value=0)\n",
    "\n",
    "        hidden_states = self.conv(hidden_states)\n",
    "        return hidden_states\n",
    "    \n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            groups,\n",
    "            eps,\n",
    "            use_in_shortcut: Optional[bool] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.norm1 = torch.nn.GroupNorm(num_groups=groups, num_channels=in_channels, eps=eps, affine=True)\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.norm2 = torch.nn.GroupNorm(num_groups=groups_out, num_channels=out_channels, eps=eps, affine=True)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.nonlinearity = Swish()\n",
    "\n",
    "\n",
    "        if self.up:\n",
    "\n",
    "\n",
    "        self.use_in_shortcut = self.in_channels != out_channels if use_in_shortcut is None else use_in_shortcut\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        GroupNorm > Swish > Conv > UpSample DownSample > GroupNorm > Swish > Conv > Shortcut\n",
    "        \"\"\"\n",
    "        hidden_states = x\n",
    "        hidden_states = self.norm1(hidden_states)\n",
    "        hidden_states = self.nonlinearity(hidden_states)\n",
    "\n",
    "        if self.upsample is not None:\n",
    "        elif self.downsample is not None:\n",
    "\n",
    "        hidden_states = self.conv1(hidden_states)\n",
    "\n",
    "        hidden_states = self.norm2(hidden_states)\n",
    "        hidden_states = self.nonlinearity(hidden_states)\n",
    "\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.conv2(hidden_states)\n",
    "\n",
    "        if self.conv_shortcut is not None:\n",
    "            x = self.conv_shortcut(x)\n",
    "\n",
    "        output_tensor = (x + hidden_states)\n",
    "        return output_tensor\n",
    "\n",
    "\n",
    "class DownEncoderBlock(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        \n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            block_out_channels: Tuple[int, ...] = (64,),\n",
    "\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv_in = nn.Conv2d(\n",
    "            in_channels,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "        )\n",
    "\n",
    "        # down\n",
    "        self.down_blocks = nn.ModuleList([])\n",
    "\n",
    "\n",
    "        # out\n",
    "        self.conv_norm_out = nn.GroupNorm(num_channels=block_out_channels[-1], num_groups=norm_num_groups, eps=1e-6)\n",
    "        self.conv_act = nn.SiLU()\n",
    "        self.conv_out = nn.Conv2d(block_out_channels[-1], out_channels, 3, padding=1)\n",
    "\n",
    "\n",
    "    def forward(self, sample):\n",
    "        sample = self.conv_in(sample)\n",
    "        # down\n",
    "        for down_block in self.down_blocks:\n",
    "            sample = down_block(sample)\n",
    "\n",
    "        # post-process\n",
    "        sample = self.conv_norm_out(sample)\n",
    "        sample = self.conv_act(sample)\n",
    "        sample = self.conv_out(sample)\n",
    "\n",
    "        return sample\n",
    "    \n",
    "\n",
    "class Decoder(nn.Module):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQVAE(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_channels=3,\n",
    "        embedding_dim=64,\n",
    "        num_embeddings=512,\n",
    "        commitment_cost=0.25,\n",
    "        hidden_channels=128,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "        \n",
    "        self.encoder = Encoder()\n",
    "        self.quant_conv = nn.Conv2d(latent_channels, vq_embed_dim, 1)\n",
    "        self.vq = VectorQuantizer()\n",
    "        self.post_quant_conv = nn.Conv2d(vq_embed_dim, latent_channels, 1)\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        h = self.quant_conv(h)\n",
    "        return h\n",
    "        \n",
    "    def decode(self, h):\n",
    "        z_q, vq_loss, _, perplexity = self.vq(h)\n",
    "        z_q2 = self.post_quant_conv(z_q)\n",
    "        dec = self.decoder(z_q2)\n",
    "        return dec, vq_loss, perplexity\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.encode(x)\n",
    "        x_recon, vq_loss, perplexity = self.decode(h)\n",
    "        return x_recon, vq_loss, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "\n",
    "batch_size = 256\n",
    "num_training_updates = 10000\n",
    "num_hiddens = 128\n",
    "num_residual_hiddens = 32\n",
    "num_residual_layers = 2\n",
    "embedding_dim = 64\n",
    "num_embeddings = 512\n",
    "commitment_cost = 0.25\n",
    "decay = 0.99\n",
    "learning_rate = 1e-3\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "training_data = datasets.CIFAR10(root=\"/home/aiteam/tykim/generative_model/data\", train=True, download=True,\n",
    "                                  transform=transforms.Compose([\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize((0.5,0.5,0.5), (1.0,1.0,1.0))\n",
    "                                  ]))\n",
    "\n",
    "validation_data = datasets.CIFAR10(root=\"/home/aiteam/tykim/generative_model/data\", train=False, download=True,\n",
    "                                  transform=transforms.Compose([\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize((0.5,0.5,0.5), (1.0,1.0,1.0))\n",
    "                                  ]))\n",
    "\n",
    "\n",
    "training_loader = DataLoader(training_data, \n",
    "                             batch_size=batch_size, \n",
    "                             shuffle=True,\n",
    "                             pin_memory=True)\n",
    "\n",
    "validation_loader = DataLoader(validation_data,\n",
    "                               batch_size=32,\n",
    "                               shuffle=True,\n",
    "                               pin_memory=True)\n",
    "\n",
    "\n",
    "model = VQVAE(embedding_dim=embedding_dim, num_embeddings=num_embeddings, commitment_cost=commitment_cost).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, amsgrad=False)\n",
    "\n",
    "\n",
    "model.train()\n",
    "train_res_recon_error = []\n",
    "train_res_perplexity = []\n",
    "\n",
    "for i in range(num_training_updates):\n",
    "    (data, _) = next(iter(training_loader))\n",
    "    data = data.to(device)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    data_recon, vq_loss, perplexity = model(data)\n",
    "    recon_error = F.mse_loss(data_recon, data, reduction='mean') * data.shape[0]\n",
    "    loss =  recon_error + vq_loss * data.shape[0]\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    train_res_recon_error.append(recon_error.item())\n",
    "    train_res_perplexity.append(perplexity.item())\n",
    "\n",
    "    if (i+1) % 100 == 0:\n",
    "        print('%d iterations' % (i+1))\n",
    "        print('recon_error: %.3f' % np.mean(train_res_recon_error[-100:]))\n",
    "        print('perplexity: %.3f' % np.mean(train_res_perplexity[-100:]))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "(valid_originals, _) = next(iter(validation_loader))\n",
    "valid_originals = valid_originals.to(device)\n",
    "\n",
    "vq_output_eval = model._pre_vq_conv(model._encoder(valid_originals))\n",
    "_, valid_quantize, _, _ = model._vq_vae(vq_output_eval)\n",
    "valid_reconstructions = model._decoder(valid_quantize)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
