{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from einops import rearrange, repeat, pack, unpack\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "import lovely_tensors as lt\n",
    "lt.monkey_patch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /home/aiteam/tykim/generative_model/data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feb1a7b63acc4b44a9b23b8623f4a8a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170498071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/aiteam/tykim/generative_model/data/cifar-10-python.tar.gz to /home/aiteam/tykim/generative_model/data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "training_data = datasets.CIFAR10(root=\"/home/aiteam/tykim/generative_model/data\", \n",
    "                                 train=True, \n",
    "                                 download=True,\n",
    "                                  transform=transforms.Compose([\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize((0.5,0.5,0.5), (1.0,1.0,1.0))\n",
    "                                  ]))\n",
    "\n",
    "validation_data = datasets.CIFAR10(root=\"/home/aiteam/tykim/generative_model/data\",\n",
    "                                   train=False, \n",
    "                                   download=True,\n",
    "                                  transform=transforms.Compose([\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize((0.5,0.5,0.5), (1.0,1.0,1.0))\n",
    "                                  ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06328692405746414"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.var(training_data.data / 255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, commitment_cost):\n",
    "        super().__init__()\n",
    "        self._embedding_dim = embedding_dim\n",
    "        self._num_embeddings = num_embeddings\n",
    "\n",
    "        self._embedding = nn.Embedding(self._num_embeddings, self._embedding_dim)\n",
    "        self._embedding.weight.data.uniform_(-1/self._num_embeddings, 1/self._num_embeddings)\n",
    "        self._commitment_cost = commitment_cost\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # B, C, H, W => B, H, W, C\n",
    "        inputs = inputs.permute(0, 2, 3, 1).contiguos()\n",
    "        input_shape = inputs.shape\n",
    "\n",
    "        #  B, H, W, C => BHW, C\n",
    "        flat_input = inputs.view(-1, self._embedding_dim)\n",
    "        # BHW, C\n",
    "        distances = (torch.sum(flat_input**2, dim=1, keepdim=True) +\n",
    "                     torch.sum(self._embedding.weight**2, dim=1)\n",
    "                     -2 * torch.matmul(flat_input, self._embedding.weight.t()))\n",
    "\n",
    "        # Encoding\n",
    "        # BHW, 1\n",
    "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n",
    "        encodings = torch.zeros(encoding_indices.shape[0], self._num_embeddings, device=inputs.device)\n",
    "        encodings.scatter_(1, encoding_indices, 1)\n",
    "\n",
    "        # Quantize and unflatten\n",
    "        quantized = torch.matmul(encodings, self._embedding.weight).view(input_shape)\n",
    "\n",
    "        # Loss\n",
    "        e_latent_loss = F.mse_loss(quantized.detach(), inputs)\n",
    "        q_latent_loss = F.mse_loss(quantized, inputs.detach())\n",
    "        loss = q_latent_loss + self._commitment_cost * e_latent_loss\n",
    "\n",
    "        quantized = inputs + (quantized - inputs).detach()\n",
    "        avg_probs = torch.mean(encodings, dim=0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorQuantizerEMA(nn.Module):\n",
    "    def __init__(self, num_embeddings, commitment_cost, decay, epsilon=1e-5):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 3])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmin(torach.tensor([[1,2,3,4,],[5,4,3,2]]), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  2,  3,  4,  5],\n",
       "        [ 6,  7,  8,  9, 10]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src = torch.arange(1, 11).reshape((2,5))\n",
    "src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 4, 0],\n",
       "        [0, 2, 0, 0, 0],\n",
       "        [0, 0, 3, 0, 0]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = torch.tensor([[0, 1, 2, 0]])\n",
    "torch.zeros(3, 5, dtype=src.dtype).scatter_(0, index, src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorQuantize(nn.Module):\n",
    "    def __init__(self, dim, codebook_size, heads=1):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "        codebook_dim = default(codebook_dim, dim)\n",
    "        self.project_in = nn.Linear(dim, codebook_input_dim) if requires_projection else nn.Identity()\n",
    "        self.project_out = nn.Linear(codebook_size)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        only_one = x.ndim == 2\n",
    "        if only_one:\n",
    "            x = rearrange(x, 'b d -> b 1 d')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 3\n",
    "codebook_input_dim = 10\n",
    "requires_projection = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EuclideanCodebook(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        codebook_size,\n",
    "        num_codebooks = 1,\n",
    "        kmeans_init = False,\n",
    "        kmeans_iters = 10,\n",
    "        sync_kmeans = True,\n",
    "        decay = 0.8,\n",
    "        eps = 1e-5,\n",
    "        threshold_ema_dead_code = 2,\n",
    "        use_ddp = False,\n",
    "        learnable_codebook = False,\n",
    "        sample_codebook_temp = 0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.decay = decay\n",
    "        init_fn = uniform_init if not kmeans_init else torch.zeros\n",
    "        embed = init_fn(num_codebooks, codebook_size, dim)\n",
    "\n",
    "        self.codebook_size = codebook_size\n",
    "        self.num_codebooks = num_codebooks\n",
    "\n",
    "        self.kmeans_iters = kmeans_iters\n",
    "        self.eps = eps\n",
    "        self.threshold_ema_dead_code = threshold_ema_dead_code\n",
    "        self.sample_codebook_temp = sample_codebook_temp\n",
    "\n",
    "        assert not (use_ddp and num_codebooks > 1 and kmeans_init), 'kmeans init is not compatible with multiple codebooks in distributed environment for now'\n",
    "\n",
    "        self.sample_fn = sample_vectors_distributed if use_ddp and sync_kmeans else batched_sample_vectors\n",
    "        self.kmeans_all_reduce_fn = distributed.all_reduce if use_ddp and sync_kmeans else noop\n",
    "        self.all_reduce_fn = distributed.all_reduce if use_ddp else noop\n",
    "\n",
    "        self.register_buffer('initted', torch.Tensor([not kmeans_init]))\n",
    "        self.register_buffer('cluster_size', torch.zeros(num_codebooks, codebook_size))\n",
    "        self.register_buffer('embed_avg', embed.clone())\n",
    "\n",
    "        self.learnable_codebook = learnable_codebook\n",
    "        if learnable_codebook:\n",
    "            self.embed = nn.Parameter(embed)\n",
    "        else:\n",
    "            self.register_buffer('embed', embed)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def init_embed_(self, data):\n",
    "        if self.initted:\n",
    "            return\n",
    "\n",
    "        embed, cluster_size = kmeans(\n",
    "            data,\n",
    "            self.codebook_size,\n",
    "            self.kmeans_iters,\n",
    "            sample_fn = self.sample_fn,\n",
    "            all_reduce_fn = self.kmeans_all_reduce_fn\n",
    "        )\n",
    "\n",
    "        self.embed.data.copy_(embed)\n",
    "        self.embed_avg.data.copy_(embed.clone())\n",
    "        self.cluster_size.data.copy_(cluster_size)\n",
    "        self.initted.data.copy_(torch.Tensor([True]))\n",
    "\n",
    "    def replace(self, batch_samples, batch_mask):\n",
    "        for ind, (samples, mask) in enumerate(zip(batch_samples.unbind(dim = 0), batch_mask.unbind(dim = 0))):\n",
    "            if not torch.any(mask):\n",
    "                continue\n",
    "\n",
    "            sampled = self.sample_fn(rearrange(samples, '... -> 1 ...'), mask.sum().item())\n",
    "            self.embed.data[ind][mask] = rearrange(sampled, '1 ... -> ...')\n",
    "\n",
    "    def expire_codes_(self, batch_samples):\n",
    "        if self.threshold_ema_dead_code == 0:\n",
    "            return\n",
    "\n",
    "        expired_codes = self.cluster_size < self.threshold_ema_dead_code\n",
    "\n",
    "        if not torch.any(expired_codes):\n",
    "            return\n",
    "\n",
    "        batch_samples = rearrange(batch_samples, 'h ... d -> h (...) d')\n",
    "        self.replace(batch_samples, batch_mask = expired_codes)\n",
    "\n",
    "    @autocast(enabled = False)\n",
    "    def forward(self, x):\n",
    "        needs_codebook_dim = x.ndim < 4\n",
    "\n",
    "        x = x.float()\n",
    "\n",
    "        if needs_codebook_dim:\n",
    "            x = rearrange(x, '... -> 1 ...')\n",
    "\n",
    "        shape, dtype = x.shape, x.dtype\n",
    "        flatten = rearrange(x, 'h ... d -> h (...) d')\n",
    "\n",
    "        self.init_embed_(flatten)\n",
    "\n",
    "        embed = self.embed if not self.learnable_codebook else self.embed.detach()\n",
    "\n",
    "        dist = -torch.cdist(flatten, embed, p = 2)\n",
    "\n",
    "        embed_ind = gumbel_sample(dist, dim = -1, temperature = self.sample_codebook_temp)\n",
    "        embed_onehot = F.one_hot(embed_ind, self.codebook_size).type(dtype)\n",
    "        embed_ind = embed_ind.view(*shape[:-1])\n",
    "\n",
    "        quantize = batched_embedding(embed_ind, self.embed)\n",
    "\n",
    "        if self.training:\n",
    "            cluster_size = embed_onehot.sum(dim = 1)\n",
    "\n",
    "            self.all_reduce_fn(cluster_size)\n",
    "            self.cluster_size.data.lerp_(cluster_size, 1 - self.decay)\n",
    "\n",
    "            embed_sum = einsum('h n d, h n c -> h c d', flatten, embed_onehot)\n",
    "            self.all_reduce_fn(embed_sum.contiguous())\n",
    "            self.embed_avg.data.lerp_(embed_sum, 1 - self.decay)\n",
    "\n",
    "            cluster_size = laplace_smoothing(self.cluster_size, self.codebook_size, self.eps) * self.cluster_size.sum()\n",
    "\n",
    "            embed_normalized = self.embed_avg / rearrange(cluster_size, '... -> ... 1')\n",
    "            self.embed.data.copy_(embed_normalized)\n",
    "            self.expire_codes_(x)\n",
    "\n",
    "        if needs_codebook_dim:\n",
    "            quantize, embed_ind = map(lambda t: rearrange(t, '1 ... -> ...'), (quantize, embed_ind))\n",
    "\n",
    "        return quantize, embed_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_in = nn.Linear(dim, codebook_input_dim) if requires_projection else nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image forward test\n",
    "x = torch.randn(32, 3, 16, 16)\n",
    "x = rearrange(x, 'b c h w -> b (h w) c')\n",
    "x = project_in(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 10])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.empty((3,4,10))\n",
    "nn.init.kaiming_uniform_(t).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform_init(*shape):\n",
    "    t = torch.empty(shape)\n",
    "    nn.init.kaiming_uniform_(t)\n",
    "    return t\n",
    "\n",
    "\n",
    "num_codebooks = 10\n",
    "codebook_size = 10\n",
    "dim = 256\n",
    "embed = uniform_init(num_codebooks, codebook_size, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQ(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    num_embeddings = 10\n",
    "    embedding_dims = 32\n",
    "    self._embedding = nn.Embedding(num_embeddings, embedding_dims)\n",
    "    self._embedding.weight.data.uniform_(-1/num_embeddings, 1/num_embeddings)\n",
    "    \n",
    "  def forward(self, x):\n",
    "    # embedidng_dim이 x의 shape와 똑같다고 가정\n",
    "    x = x.permute(0, 2, 3, 1).contiguous()\n",
    "    x = x.view(-1, embedding_dims)\n",
    "    \n",
    "    distances = torch.sum(x**2, dim=1, keepdim=True) + torch.sum(self._embedding.weight**2, dim=1) - 2*torch.matmul(x, self._embedding.weight.t())\n",
    "    \n",
    "    encoding_indicies = torch.argmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[10, 5] n=50 x∈[33.163, 94.940] μ=57.484 σ=12.957"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(10, 32)\n",
    "emb = torch.randn(5, 32)\n",
    "\n",
    "\n",
    "sim = torch.matmul(x, emb.t())\n",
    "\n",
    "# measure l2-normalized distance btw flattened encoder output and code words\n",
    "\n",
    "dist = torch.sum(x**2, dim=1, keepdim=True) + torch.sum(emb**2, dim=1) -2 * sim\n",
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[10, 5] n=50 x∈[27.652, 46.419] μ=36.437 σ=4.741"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cdist(x, emb, p=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist.shape\n",
    "torch.argmin(dist, dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[10] i64 x∈[0, 4] μ=1.500 σ=1.650 [1, 0, 0, 3, 0, 4, 0, 2, 1, 4]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmin(dist, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[10, 1] i64 x∈[0, 4] μ=1.500 σ=1.650 [[1], [0], [0], [3], [0], [4], [0], [2], [1], [4]]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmin(dist, dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[10, 1] i64 x∈[0, 4] μ=1.500 σ=1.650 [[1], [0], [0], [3], [0], [4], [0], [2], [1], [4]]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmin(dist, dim=1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_indicies = torch.argmin(dist, dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[10, 5] \u001b[38;2;127;127;127mall_zeros\u001b[0m"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings = torch.zeros(10, 5)\n",
    "encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[10, 5] n=50 x∈[0., 1.000] μ=0.200 σ=0.404\n",
       "tensor([[0., 1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one-hot vector 처럼 만듬\n",
    "encodings.scatter_(1, encoding_indicies, 1).v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[1, 10, 32] n=320 x∈[-2.581, 2.787] μ=-0.053 σ=0.998"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(encodings, emb).view(1, 10, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[10, 5] n=50 x∈[33.163, 94.940] μ=57.484 σ=12.957"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist = torch.sum(x**2, dim=1, keepdim=True) + torch.sum(emb**2, dim=1) -2 * sim\n",
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor 16.544"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(x, p=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQ(nn.Module):\n",
    "  def __init__(self):\n",
    "    self.emb = nn.Embedding(num_embeddings, embedding_dim)\n",
    "    \n",
    "  def forward(self, x):\n",
    "    x = rearrange(x , 'b c h w -> (b h w) c')\n",
    "    distannces = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Index tensor must have the same number of dimensions as src tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/aiteam/tykim/generative_model/framework/vae/vqvae/model.ipynb Cell 32\u001b[0m in \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B211.168.94.221/home/aiteam/tykim/generative_model/framework/vae/vqvae/model.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m new_color \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([[\u001b[39m5\u001b[39m]])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B211.168.94.221/home/aiteam/tykim/generative_model/framework/vae/vqvae/model.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# Use scatter_ to change the color of the specified balls\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B211.168.94.221/home/aiteam/tykim/generative_model/framework/vae/vqvae/model.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m ball_colors\u001b[39m.\u001b[39;49mscatter_(\u001b[39m0\u001b[39;49m, indices_to_change, new_color)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B211.168.94.221/home/aiteam/tykim/generative_model/framework/vae/vqvae/model.ipynb#X43sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mprint\u001b[39m(ball_colors)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Index tensor must have the same number of dimensions as src tensor"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a tensor with the original ball colors\n",
    "ball_colors = torch.tensor([0, 1, 2, 3, 4, 2, 1, 0])\n",
    "\n",
    "# Create a tensor with the indices of the balls you want to change\n",
    "indices_to_change = torch.tensor([1, 3, 6])\n",
    "\n",
    "# Create a tensor with the new color you want to change the balls to\n",
    "new_color = torch.tensor([[5]])\n",
    "\n",
    "# Use scatter_ to change the color of the specified balls\n",
    "ball_colors.scatter_(0, indices_to_change, new_color)\n",
    "\n",
    "print(ball_colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5349, 0.1988, 0.6592, 0.6569, 0.2328],\n",
       "        [0.4251, 0.2071, 0.6297, 0.3653, 0.8513]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(7)\n",
    "\n",
    "x = torch.rand(2, 5)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.zeros(3,5)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5349, 0.2071, 0.6297, 0.6569, 0.2328],\n",
      "        [0.0000, 0.1988, 0.0000, 0.3653, 0.8513],\n",
      "        [0.4251, 0.0000, 0.6592, 0.0000, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "print(y.scatter_(0,torch.LongTensor([[0, 1, 2, 0, 0], [2, 0, 0, 1, 1]]),x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[2, 5] n=10 x∈[0.199, 0.851] μ=0.476 σ=0.225 [[0.535, 0.199, 0.659, 0.657, 0.233], [0.425, 0.207, 0.630, 0.365, 0.851]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(7)\n",
    "\n",
    "x = torch.rand(2, 5)\n",
    "x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[3, 5] \u001b[38;2;127;127;127mall_zeros\u001b[0m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.zeros(3,5)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor[3, 5] n=15 x∈[0., 1.000] μ=0.667 σ=0.488\n",
       "tensor([[1., 1., 1., 1., 1.],\n",
       "        [0., 1., 0., 1., 1.],\n",
       "        [1., 0., 1., 0., 0.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.scatter_(0,torch.LongTensor([[0, 1, 2, 0, 0], [2, 0, 0, 1, 1]]),1).v"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3dfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0041225012c46a81f6ea3650572d975902102d8f41a1704402cdfe5d667efe52"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
