{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_torch2_available():\n",
    "  from .atten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resampler(nn.Module):\n",
    "  def __init__(\n",
    "      self, \n",
    "      dim=1024,\n",
    "      depth=8, \n",
    "      dim_head=64,\n",
    "      heads=16,\n",
    "      num_quries=8,\n",
    "      embedding_dim=768,\n",
    "      output_dim=1024,\n",
    "      ff_mult=4\n",
    "      ):\n",
    "    super().__init__()\n",
    "\n",
    "    self.latents = nn.Parameter(torch.randn(1, num_quries, dim)/dim**0.5)\n",
    "    self.proj_in = nn.Linear(embedding_dim, dim)\n",
    "    self.proj_out = nn.Linear(dim, output_dim)\n",
    "    self.norm_out = nn.LayerNorm(output_dim)\n",
    "\n",
    "    self.layers = nn.ModuleList([])\n",
    "\n",
    "  def forward(self, x):\n",
    "    latents = self."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageProjModel(nn.Module):\n",
    "  def __init__(self, cross_attention_dim=1024, clip_embeddings_dim=1024, clip_extra_context_tokens=4):\n",
    "    super().__init__()\n",
    "\n",
    "    self.cross_attention_dim = cross_attention_dim\n",
    "    self.clip_extra_context_tokens = clip_extra_context_tokens\n",
    "    self.proj = nn.Linear(clip_embeddings_dim, self.clip_extra_context_tokens * cross_attention_dim)\n",
    "    self.norm = torch.nn.LayerNorm(cross_attention_dim)\n",
    "\n",
    "  def forward(self, image_embeds):\n",
    "    embeds = image_embeds\n",
    "    clip_extra_context_tokens = self.proj(embeds).reshape(-1, self.clip_extra_context_tokens, self.cross_attention_dim)\n",
    "    clip_extra_context_tokens = self.norm(clip_extra_context_tokens)\n",
    "    return clip_extra_context_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IPAdapter:\n",
    "    def __init__(self, sd_pipe, image_encoder_path, ip_ckpt, device, num_tokens=4):\n",
    "        self.device = device\n",
    "        self.image_encoder_path = image_encoder_path\n",
    "        self.ip_ckpt = ip_ckpt\n",
    "        self.num_tokens = num_tokens\n",
    "\n",
    "        self.pipe = sd_pipe.to(self.device)\n",
    "        self.set_ip_adapter()\n",
    "\n",
    "        self.image_encoder = self.init_proj()\n",
    "\n",
    "        self.image_encoder = CLIPVisionModelWithProjection.from_pretrained(self.image_encoder_path).to(self.device, dtype=torch.float16)\n",
    "        self.clip_image_processor = CLIPImageProcessor()\n",
    "\n",
    "        self.image_proj_model = self.init_proj()\n",
    "\n",
    "        self.load_ip_adapter()\n",
    "        \n",
    "    def init_proj(self):\n",
    "        image_proj_model = ImageProjModel(\n",
    "            cross_attention_dim=self.pipe.unet.config.cross_attention_dim,\n",
    "            clip_embeddings_dim=self.image_encoder.config.projection_dim,\n",
    "            clip_extra_context_tokens=self.num_tokens,\n",
    "        ).to(self.device, dtype=torch.float16)\n",
    "        return image_proj_model\n",
    "\n",
    "    def set_ip_adapter(self):\n",
    "        unet = self.pipe.unet\n",
    "        attn_procs = {}\n",
    "        # moduel list라서 index 번호 있음\n",
    "        # dict_keys(['down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor', 'down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor', 'down_blocks.0.attentions.1.transformer_blocks.0.attn1.processor', 'down_blocks.0.attentions.1.transformer_blocks.0.attn2.processor', 'down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor', 'down_blocks.1.attentions.0.transformer_blocks.0.attn2.processor', 'down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor', 'down_blocks.1.attentions.1.transformer_blocks.0.attn2.processor', 'down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor', 'down_blocks.2.attentions.0.transformer_blocks.0.attn2.processor', 'down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor', 'down_blocks.2.attentions.1.transformer_blocks.0.attn2.processor', 'up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor', 'up_blocks.1.attentions.0.transformer_blocks.0.attn2.processor', 'up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor', 'up_blocks.1.attentions.1.transformer_blocks.0.attn2.processor', 'up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor', 'up_blocks.1.attentions.2.transformer_blocks.0.attn2.processor', 'up_blocks.2.attentions.0.transformer_blocks.0.attn1.processor', 'up_blocks.2.attentions.0.transformer_blocks.0.attn2.processor', 'up_blocks.2.attentions.1.transformer_blocks.0.attn1.processor', 'up_blocks.2.attentions.1.transformer_blocks.0.attn2.processor', 'up_blocks.2.attentions.2.transformer_blocks.0.attn1.processor', 'up_blocks.2.attentions.2.transformer_blocks.0.attn2.processor', 'up_blocks.3.attentions.0.transformer_blocks.0.attn1.processor', 'up_blocks.3.attentions.0.transformer_blocks.0.attn2.processor', 'up_blocks.3.attentions.1.transformer_blocks.0.attn1.processor', 'up_blocks.3.attentions.1.transformer_blocks.0.attn2.processor', 'up_blocks.3.attentions.2.transformer_blocks.0.attn1.processor', 'up_blocks.3.attentions.2.transformer_blocks.0.attn2.processor', 'mid_block.attentions.0.transformer_blocks.0.attn1.processor', 'mid_block.attentions.0.transformer_blocks.0.attn2.processor'])\n",
    "        for name in unet.attn_processor.keys():\n",
    "            cross_attention_dim = None if name.endswith(\"attn1.processor\") else unet.config.cross_attention_dim\n",
    "\n",
    "            if name.startswith('mid_block'):\n",
    "                hidden_size = unet.config.block_out_channels[-1]\n",
    "            elif name.startswith('up_blocks'):\n",
    "                block_id = int(name[len])\n",
    "\n",
    "    def load_ip_adapter(self):\n",
    "        state_dict = torch.load(self.ip_ckpt, map_location='cpu')\n",
    "        self.image_proj_model.load_state_dict(state_dict['image_proj'])\n",
    "        ip_layers = torch.nn.ModuleList(self.pipe.unet.attn_processors.values())\n",
    "        ip_layers.load_state_dict(state_dict['ip_adapter'])\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def get_image_embeds(self, pil_image):\n",
    "        if isinstance(pil_image, Image.Image):\n",
    "            pil_image = [pil_image]\n",
    "\n",
    "        # CLIP Processor\n",
    "        clip_image = self.clip_image_processor(images=pil_image, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "        # CLIP 인코더\n",
    "        clip_image_embeds = self.image_encoder(clip_image.to(self.device, dtye=torch.float16)).image_embeds\n",
    "\n",
    "        # projection \n",
    "        image_prompt_embeds = self.image_proj_model(clip_image_embeds)\n",
    "        uncond_image_prompt_embeds = self.image_proj_model(torch.zeros_like(clip_image_embeds))\n",
    "        return image_prompt_embeds, uncond_image_prompt_embeds\n",
    "\n",
    "    def set_scale(self, scale):\n",
    "        for attn_processor in self.pipe.unet.attn_processors.values():\n",
    "            if isinstance(attn_processor, IPAttnProcessor):\n",
    "                attn_processor.scale = scale\n",
    "\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        pil_image,\n",
    "        prompt=None,\n",
    "        negative_prompt=None,\n",
    "        scale=1.0,\n",
    "        \n",
    "    ):\n",
    "        self.set_scale(scale)\n",
    "\n",
    "        if isinstance(pil_image, Image.Image):\n",
    "            num_prompts = 1\n",
    "        else:\n",
    "            num_prompts = len(pil_image)\n",
    "\n",
    "        if prompt is None:\n",
    "            prompt = \"best quality, high quality\"\n",
    "        if negative_prompt is None:\n",
    "            negative_prompt = \"monochrome, lowres, bad anatomy, worst quality, low quality\"\n",
    "\n",
    "        if not isinstance(prompt, List):\n",
    "            prompt = [prompt] * num_prompts\n",
    "        if not isinstance(negative_prompt, List):\n",
    "            negative_prompt = [negative_prompt] * num_prompts\n",
    "\n",
    "\n",
    "        image_prompt_embeds, uncond_image_prompt_embeds = self.get_image_embeds(pil_image)\n",
    "        bs_embed, seq_len, _ = image_prompt_embeds.shape\n",
    "\n",
    "        image_prompt_embeds = image_prompt_embeds.repeat(1, num_samples, 1)\n",
    "        image_prompt_embeds = image_prompt_embeds.view(bs_embed * num_samples, seq_len, -1)\n",
    "\n",
    "        uncond_image_prompt_embeds = uncond_image_prompt_embeds.repeat(1, num_samples, 1)\n",
    "        uncond_image_prompt_embeds = uncond_image_prompt_embeds.view(bs_embed * num_samples,\n",
    "                                                                     seq_len,\n",
    "                                                                     -1)\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            prompt_embeds = self.pipe._encode_prompt(\n",
    "                prompt, device=self.device, num_images_per_prompt=num_samples, do_classifier_free_guidance=True, negative_prompt=negative_prompt)\n",
    "            negative_prompt_embeds_, prompt_embeds_ = prompt_embeds.chunk(2)\n",
    "            prompt_embeds = torch.cat([prompt_embeds_, image_prompt_embeds], dim=1)\n",
    "            negative_prompt_embeds = torch.cat([negative_prompt_embeds_, uncond_image_prompt_embeds], dim=1)\n",
    "\n",
    "        generator = torch.Generator(self.device).manual_seed(seed) if seed is not None else None\n",
    "        images = self.pipe(\n",
    "            prompt_embeds=prompt_embeds,\n",
    "            negative_prompt_embeds=negative_prompt_embeds,\n",
    "            guidance_scale=guidance_scale,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            generator=generator,\n",
    "            **kwargs,\n",
    "        ).images\n",
    "\n",
    "        return images\n",
    "\n",
    "\n",
    "        \n",
    "    def set_ip_adapter(self):\n",
    "        unet = self.pipe.unet\n",
    "        attn_procs = {}\n",
    "        for name in unet.attn_processors.keys():\n",
    "        cross_attention_dim = None if name.endswith(\"attn1.processor\") else unet.config.cross_attention_dim\n",
    "\n",
    "    \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IPAdapterXL(IPAdapter):\n",
    "    def generate(\n",
    "        self,\n",
    "        pil\n",
    "    ):\n",
    "        self.set_scale(scale)\n",
    "\n",
    "        if isinstance(pil_image, Image.Image):\n",
    "            num_prompts = 1\n",
    "        else:\n",
    "            num_prompts = len(pil_image)\n",
    "\n",
    "        if prompt is None:\n",
    "            prompt = \"best quality, high quality\"\n",
    "        if negative_prompt is None:\n",
    "            negative_prompt = \"monochrome, lowres, bad anatomy, worst quality, low quality\"\n",
    "\n",
    "        if not isinstance(prompt, List):\n",
    "            prompt = [prompt] * num_prompts\n",
    "        if not isinstance(negative_prompt, List):\n",
    "            negative_prompt = [negative_prompt] * num_prompts\n",
    "\n",
    "        image_prompt_embeds, uncond_image_prompt_embeds = self.get_image_embeds(pil_image)\n",
    "        bs_embed, seq_len, _ = image_prompt_embeds.shape\n",
    "        image_prompt_embeds = image_prompt_embeds.repeat(1, num_samples, 1)\n",
    "        image_prompt_embeds = image_prompt_embeds.view(bs_embed * num_samples, seq_len, -1)\n",
    "\n",
    "        uncond_image_prompt_embeds = uncond_image_prompt_embeds.repeat(1, num_samples, 1)\n",
    "        uncond_image_prompt_embeds = uncond_image_prompt_embeds.view(bs_embed * num_samples, seq_len, -1)\n",
    "\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            prompt_embeds, negative_prompt_embeds, pooled_prompt_embeds, negative_pooled_prompt_embeds = self.pipe.encode_prompt(\n",
    "                prompt, num_images_per_prompt=num_samples, do_classifier_free_guidance=True, negative_prompt=negative_prompt)\n",
    "\n",
    "            \n",
    "        generator = torch.Generator(self.device).manual_seed(seed) if seed is not None else None\n",
    "        images = self.pipe(\n",
    "            prompt_embeds = prompt_embeds,\n",
    "            negative_prompt_embeds=negative_prompt_embeds,\n",
    "            pooled_prompt_embeds=pooled_prompt_embeds,\n",
    "            negative_pooled_prompt_embeds=negative_pooled_prompt_embeds,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            generator=generator,\n",
    "            **kwargs,\n",
    "        ).images\n",
    "\n",
    "        return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IPAdapterPlus(IPAdapter):\n",
    "    def init_proj(self):\n",
    "        image_proj_model = Resampler(\n",
    "            dim=self.pipe.unet.config.cross_attention_dim,\n",
    "            depth=4,\n",
    "            dim_head=64,\n",
    "            heads=12,\n",
    "            \n",
    "            ).to(self.device, dtype=torch.float16)\n",
    "        return image_proj_model\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def get_image_embeds(self, pil_image):\n",
    "        if isinstance(pil_image, Image.Image):\n",
    "            pil_image = [pil_image]\n",
    "\n",
    "        # CLIP\n",
    "        clip_image = self.clip_image_processor(images=pil_image, return_tensors=\"pt\").pixel_values\n",
    "        clip_image = clip_image.to(self.device, dtype=torch.float16)\n",
    "        \n",
    "        # CLIP encoder\n",
    "        # 달라진건 image embedding이 아니라 hidden states를 가지고 온다\n",
    "        # CLIPEncoder에서 얻은 hidden_states를 가지고 옴\n",
    "        # CLIPEncoderLayer마다 거쳐서 나오는 intermediate feature 이용\n",
    "        clip_image_embeds = self.image_encoder(clip_image, output_hidden_states=True).hidden_states[-2]\n",
    "\n",
    "        image_prompt_embeds = self.image_proj_model(clip_image_embeds)\n",
    "\n",
    "        uncond_clip_image_embeds = self.image_encoder(torch.zeros_like(clip_image), output_hidden_states=True).hidden_states[-2]\n",
    "        uncond_image_prompt_embeds = self.image_proj_model(uncond_clip_image_embeds)\n",
    "\n",
    "        return image_prompt_embeds, uncond_image_prompt_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDIMSampler(object):\n",
    "    def q_sample(self, x_start, t, noise=None):\n",
    "        noise = default(noise, lambda: torch.randn_like(x_start))\n",
    "        return (extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start +\n",
    "                    extract_into_tensor(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise)\n",
    "\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample_ddim(self, x, c, t, index, repeat_noise=False,\n",
    "                      use_original_steps=False, input_noise=None,\n",
    "                      quantize_denoised=False, temperature=1., noise_dropout=0., score_corrector=None, corrector_kwargs=None,\n",
    "                      unconditional_guidance_scale=1., unconditional_conditioning=None):\n",
    "        \n",
    "\n",
    "        return x_prev, pred_x0\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def decode(self, x_latent, cond, t_start, unconditional_guiodnace_scale=1.0,\n",
    "            unconditional_conditioning=None,\n",
    "                use_original_steps=False, input_noise = None):\n",
    "        \n",
    "        timesteps = np.arange(self.ddpm_num_timesteps) if use_original_steps else self.ddim_timesteps\n",
    "        timesteps = timesteps[:t_start]\n",
    "\n",
    "        time_range = np.flip(timesteps)\n",
    "        total_steps = timesteps.shape[0]\n",
    "\n",
    "\n",
    "        x_dec = x_latent\n",
    "        for i, step in enumerate(iterator):\n",
    "            index = total_steps - i - 1\n",
    "            ts = torch.full((x_latent.shape[0],), step, device=x_latent.device, dtype=torch.long)\n",
    "            x_dec, _ = self.p_sample_ddim(x_dec, cond, ts, index=index,\n",
    "                                        use_original_steps=use_original_steps,\n",
    "                                        unconditional_conditioning_scale=unconditional_guidance_scale,\n",
    "                                        unconditional_conditioning=unconditional_conditioning,\n",
    "                                        input_noise=input_noise)\n",
    "            \n",
    "            return x_dec\n",
    "\n",
    "\n",
    "\n",
    "class EmbeddingManager(nn.Module):\n",
    "    def __init__(self,):\n",
    "\n",
    "    def load(self, ckpt_path):\n",
    "        ckpt = torch.load(ckpt_path, map_location='cpu')\n",
    "        self.string_to_token_dict = ckpt[\"string_to_token\"]\n",
    "\n",
    "        if 'attention' in ckpt.keys():\n",
    "            self.attention = ckpt[\"attention\"]\n",
    "        else:\n",
    "            self.attention = None\n",
    "\n",
    "    \n",
    "    def \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class LatentDiffusion(DDPM):\n",
    "    def __init__(self,\n",
    "                 first_stage_config,\n",
    "                 cond_stage_config,\n",
    "                 ):\n",
    "        \n",
    "\n",
    "        self.instantiate_first_stage(first_stage_config)\n",
    "        self.instantiate_cond_stage(cond_stage_config)\n",
    "\n",
    "\n",
    "        self.cond_stage_model.train = disabled_train\n",
    "        for param in self.cond_stage_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "\n",
    "        self.embedding_manager = self.instantiate_embedding_mananger(personalization_config, self.cond_stage_model)\n",
    "\n",
    "        self.emb_ckpt_counter = 0\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "\n",
    "        for param in self.embedding_manager.embedding_parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "\n",
    "    def instantiate_embedding_manager(self, config, embedder):\n",
    "        model = instantiate_from_config(config, embedder=embedder)\n",
    "\n",
    "        if config.params.get():\n",
    "            model.load(config.params.embedding_manager_ckpt)\n",
    "\n",
    "        return model\n",
    "    \n",
    "\n",
    "    def instantiate_cond_stage(self, config):\n",
    "        if not self.cond_stage_trainable:\n",
    "            if config == '__is_first_stage__':\n",
    "                self.cond_stage_model = self.first_stage_model\n",
    "            elif config == '__is_unconditional__':\n",
    "                self.cond_stage_model = None\n",
    "            else:\n",
    "                model = instantiate_from_config(config)\n",
    "                self.cond_stage_model = model.eval()\n",
    "                self.cond_stage_model.train = disabled_train\n",
    "                for param in self.cond_stage_model.parameters():\n",
    "                    param.requires_grad = False\n",
    "        else:\n",
    "            model = instantiate_from_config(config)\n",
    "            self.cond_stage_model = model\n",
    "\n",
    "\n",
    "\n",
    "    def get_learned_conditining(slef, c, x=None):\n",
    "        if slef.cond_stage_forward is None:\n",
    "            if hasattr() and callable(self.cond_stage_model.encode):\n",
    "                c = self.cond_stage_model.encode(c, embedding_manager=self.embedding_manager, input_img = x)\n",
    "                if isinstance(c, DiagonalGaussianDistribution):\n",
    "                    c = c.mode()\n",
    "            else:\n",
    "                c = self.cond_stage_model(c, input_img=x)\n",
    "        else:\n",
    "            c = getattr(self.cond_stage_model, self.cond_stage_forward)(c, input_img=x)\n",
    "        return c\n",
    "    \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        lr = self.learning_rate\n",
    "\n",
    "        if self.embedding_manager is not None:\n",
    "            params = list(self.embedding_manager.embedding_parameters())\n",
    "        else:\n",
    "            params = list(self.model.parameters())\n",
    "            if self.cond_stage_trainable:\n",
    "                params = params + list()\n",
    "\n",
    "        \n",
    "        opt = torch.optim.AdamW(params, lr=lr)\n",
    "        return opt\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "ddim_steps = 50\n",
    "strength = 0.5\n",
    "scale=10.0\n",
    "t_enc = int(strength * ddim_steps)\n",
    "\n",
    "all_samples = list()\n",
    "\n",
    "x_noisy = model.q_sample(x_start=init_latent, t=torch.tensor([t_enc]*batch_size).to(device))\n",
    "model_output = model.apply_model(x_noisy, torch.tensor([t_enc]*batch_size).to(device), c)\n",
    "z_enc = sampler.stochastic_encode(init_latent, torch.tensor([t_enc]*batch_size).to(device),\n",
    "                                  noise = model_output, use_original_steps=True)\n",
    "\n",
    "t_enc = int(strength * ddim_steps)\n",
    "# p sample\n",
    "samples = sampler.decode(z_enc, c, t_enc, \n",
    "                         unconditioanl_guidance_scale=scale,\n",
    "                         unconditional_conditioning=uc,)\n",
    "\n",
    "\n",
    "x_samples = model.decode_first_stage(samples)\n",
    "x_samples = torch.clamp((x_samples + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "\n",
    "for x_sample in x_samples:\n",
    "    x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
    "all_samples.append(x_samples)\n",
    "\n",
    "grid = torch.stack(all_samples, 0)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
