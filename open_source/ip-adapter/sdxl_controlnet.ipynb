{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, StableDiffusionImg2ImgPipeline, StableDiffusionInpaintPipelineLegacy, DDIMScheduler, AutoencoderKL\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from diffusers.pipelines.controlnet import MultiControlNetModel\n",
    "from transformers import CLIPVisionModelWithProjection, CLIPImageProcessor\n",
    "from PIL import Image\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from diffusers import StableDiffusionControlNetPipeline, DDIMScheduler, AutoencoderKL, ControlNetModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_path = \"runwayml/stable-diffusion-v1-5\"\n",
    "vae_model_path = \"stabilityai/sd-vae-ft-mse\"\n",
    "image_encoder_path = \"models/image_encoder/\"\n",
    "ip_ckpt = \"models/ip-adapter_sd15.bin\"\n",
    "device = \"cuda\"\n",
    "\n",
    "noise_scheduler = DDIMScheduler(\n",
    "    num_train_timesteps=1000,\n",
    "    beta_start=0.00085,\n",
    "    beta_end=0.012,\n",
    "    beta_schedule=\"scaled_linear\",\n",
    "    clip_sample=False,\n",
    "    set_alpha_to_one=False,\n",
    "    steps_offset=1,\n",
    ")\n",
    "vae = AutoencoderKL.from_pretrained(vae_model_path, cache_dir='/home/tyk/hf_cache').to(dtype=torch.float16)\n",
    "\n",
    "\n",
    "image_encoder = CLIPVisionModelWithProjection.from_pretrained(\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\", cache_dir='/home/tyk/hf_cache').to(device, dtype=torch.float16)\n",
    "clip_image_processor = CLIPImageProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e12c6a1fdc0a438aaaa60ea7bb32f4e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have disabled the safety checker for <class 'diffusers.pipelines.controlnet.pipeline_controlnet.StableDiffusionControlNetPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n"
     ]
    }
   ],
   "source": [
    "controlnet_model_path = \"lllyasviel/control_v11f1p_sd15_depth\"\n",
    "controlnet = ControlNetModel.from_pretrained(controlnet_model_path, torch_dtype=torch.float16, cache_dir='/home/tyk/hf_cache')\n",
    "# load SD pipeline\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    base_model_path,\n",
    "    controlnet=controlnet,\n",
    "    torch_dtype=torch.float16,\n",
    "    scheduler=noise_scheduler,\n",
    "    vae=vae,\n",
    "    feature_extractor=None,\n",
    "    safety_checker=None,\n",
    "    cache_dir='/home/tyk/hf_cache'\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "pipe.unet.config.cross_attention_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageProjModel(torch.nn.Module):\n",
    "    \"\"\"Projection Model\"\"\"\n",
    "    def __init__(self, cross_attention_dim=1024, clip_embeddings_dim=1024, clip_extra_context_tokens=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.cross_attention_dim = cross_attention_dim\n",
    "        self.clip_extra_context_tokens = clip_extra_context_tokens\n",
    "        self.proj = torch.nn.Linear(clip_embeddings_dim, self.clip_extra_context_tokens * cross_attention_dim)\n",
    "        self.norm = torch.nn.LayerNorm(cross_attention_dim)\n",
    "        \n",
    "    def forward(self, image_embeds):\n",
    "        embeds = image_embeds\n",
    "        clip_extra_context_tokens = self.proj(embeds).reshape(-1, self.clip_extra_context_tokens, self.cross_attention_dim)\n",
    "        clip_extra_context_tokens = self.norm(clip_extra_context_tokens)\n",
    "        return clip_extra_context_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.unet.config.cross_attention_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_encoder.config.projection_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_proj_model = ImageProjModel(\n",
    "            cross_attention_dim=pipe.unet.config.cross_attention_dim,\n",
    "            clip_embeddings_dim=image_encoder.config.projection_dim,\n",
    "            clip_extra_context_tokens=4,\n",
    "        ).to(device, dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.utils import load_image\n",
    "\n",
    "pil_image = load_image(\n",
    "    \"https://hf.co/datasets/huggingface/documentation-images/resolve/main/diffusers/input_image_vermeer.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_image = clip_image_processor(images=pil_image, return_tensors=\"pt\").pixel_values # [1, 3, 224, 224]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_image_embeds = image_encoder(clip_image.to(device, dtype=torch.float16)).image_embeds # [1, 1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_prompt_embeds = image_proj_model(clip_image_embeds) # [1, 4, 768]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_embed, seq_len, _ = image_prompt_embeds.shape\n",
    "image_prompt_embeds = image_prompt_embeds.repeat(1, 4, 1)\n",
    "image_prompt_embeds = image_prompt_embeds.view(bs_embed * num_samples, seq_len, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4, 768])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_prompt_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncond_image_prompt_embeds = image_proj_model(torch.zeros_like(clip_image_embeds))\n",
    "uncond_image_prompt_embeds = uncond_image_prompt_embeds.repeat(1, num_samples, 1)\n",
    "uncond_image_prompt_embeds = uncond_image_prompt_embeds.view(bs_embed * num_samples, seq_len, -1)\n",
    "# torch.Size([4, 4, 768])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4, 768])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uncond_image_prompt_embeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_prompts = 1\n",
    "num_samples = 4\n",
    "\n",
    "\n",
    "prompt = \"best quality, high quality\"\n",
    "negative_prompt = \"monochrome, lowres, bad anatomy, worst quality, low quality\"\n",
    "\n",
    "prompt = [prompt] * num_prompts\n",
    "negative_prompt = [negative_prompt] * num_prompts\n",
    "\n",
    "# torch.Size([8, 77, 768])\n",
    "prompt_embeds = pipe._encode_prompt(prompt, device=device,\n",
    "                    num_images_per_prompt=num_samples,\n",
    "                    do_classifier_free_guidance=True,\n",
    "                    negative_prompt=negative_prompt)\n",
    "\n",
    "# [4, 77, 768]\n",
    "negative_prompt_embeds_, prompt_embeds_ = prompt_embeds.chunk(2)\n",
    "\n",
    "# torch.Size([4, 77 + 4, 768])\n",
    "prompt_embeds = torch.cat([prompt_embeds_, image_prompt_embeds], dim=1)\n",
    "negative_prompt_embeds = torch.cat([negative_prompt_embeds_, uncond_image_prompt_embeds], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 77, 768])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_prompt_embeds_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class StableDiffusionControlNetPipeline():\n",
    "\n",
    "    #\n",
    "    def _encode_prompt(self, \n",
    "                       prompt,\n",
    "                       device,\n",
    "                       num_images_per_prompt,\n",
    "                       do_classifier_free_guidance,\n",
    "                       negative_prompt=None,\n",
    "                       prompt_embeds: Optional[torch.FloatTensor] = None,\n",
    "                       negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n",
    "                       lora_scale: Optional[float] = None,):\n",
    "\n",
    "        # prompt를 이용\n",
    "        if prompt_embeds is None:\n",
    "            ...\n",
    "\n",
    "            prompt_embeds = self.text_encoder(\n",
    "                text_input_ids.to(device),\n",
    "                attention_mask=attention_mask,\n",
    "            )\n",
    "            prompt_embeds = prompt_embeds[0]\n",
    "            \n",
    "\n",
    "        \n",
    "        prompt_embeds = prompt_embeds.to(dtype=self.text_encoder.dtype, device=device)\n",
    "\n",
    "        bs_embed, seq_len, _ = prompt_embeds.shape\n",
    "        prompt_embeds = prompt_embeds.repeat(1, num_images_per_prompt, 1)\n",
    "        prompt_embeds = prompt_embeds.view(bs_embed * num_images_per_prompt, seq_len, -1)\n",
    "\n",
    "        # get unconditional embeddings for classifier free guidance\n",
    "        # negative_prompt_embeds가 있어서 여긴 실행 x\n",
    "        if do_classifier_free_guidance and negative_prompt_embeds is None:\n",
    "            ...\n",
    "\n",
    "        # CFG를 하면\n",
    "        if do_classifier_free_guidance:\n",
    "            seq_len = negative_prompt_embeds.shape[1]\n",
    "\n",
    "            negative_prompt_embeds = negative_prompt_embeds.to(dtype=self.text_encoder.dtype, device=device)\n",
    "\n",
    "\n",
    "            prompt_embeds = torch.cat([])\n",
    "            \n",
    "\n",
    "        return prompt_embeds\n",
    "        \n",
    "    def __call__(self,\n",
    "                 prompt: Union[str, List[str]] = None,\n",
    "                 image,\n",
    "                 height: Optional[int] = None,\n",
    "                 width: Optional[int] = None,\n",
    "                 num_inference_step=50,\n",
    "                 prompt_embeds = None\n",
    "                 ):\n",
    "\n",
    "        # call하면 netavie랑 같이 묶여있음 : torch.Size([4, 2*(77 + 4), 768])\n",
    "        prompt_embeds = self._encode_prompt(\n",
    "            prompt,\n",
    "            device,\n",
    "            prompt_embeds=prompt_embeds,\n",
    "            negative_prompt_embeds=negative_prompt_embeds,\n",
    "            lora_scale=text_encoder_lora_scale,\n",
    "        )\n",
    "\n",
    "\n",
    "        # depth map을 \n",
    "        if isinstance(controlnet, ControlNetModel):\n",
    "            image = self.prepare_image(image=image,\n",
    "                                       ...\n",
    "            )\n",
    "\n",
    "\n",
    "        # 8. Denoising loop\n",
    "        num_warmup_steps = len(timesteps) - num_inference_steps * self.scheduler.order\n",
    "        with self.progress_bar(total=num_inference_steps) as progress_bar:\n",
    "            for i, t in enumerate(timesteps):\n",
    "                down_block_res_samples, mid_block_res_sample = self.controlnet(\n",
    "                    \n",
    "            \n",
    "        \n",
    "class IPAdapter:\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def get_image_embeds(self, pil_image):\n",
    "        if isinstance(pil_image, Image.Image):\n",
    "            pil_image = [pil_image]\n",
    "\n",
    "        \n",
    "    def generate(self,\n",
    "                 pil_image,\n",
    "                 prompt=None, # multimodal prompt에서 사용 (\"best quality, high quality, wearing a hat on the beach\")\n",
    "                 negative_prompt=None,\n",
    "                 scale=1.0,\n",
    "                 num_samples=4,\n",
    "                 seed=-1,\n",
    "                 guidance_scale=7.5,\n",
    "                 num_inference_steps=30,\n",
    "                 **kwargs, # controlnet_conditioning_scale=0.7, image=depth_map\n",
    "                 ):\n",
    "        self.set_scale(scale)\n",
    "\n",
    "        # 이미지 갯수만큼\n",
    "        if isinstance(pil_image, Image.Image):\n",
    "            num_prompts = 1\n",
    "        else:\n",
    "            num_prompts = len(pil_image)\n",
    "\n",
    "        # prompt는 항상 넣음\n",
    "        if prompt is None:\n",
    "            prompt = \"best quality, high quality\"\n",
    "        if negative_prompt is None:\n",
    "            negative_prompt = \"monochrome, lowres, bad anatomy, worst quality, low quality\"\n",
    "\n",
    "        # 이미지 갯수만큼\n",
    "        if not isinstance(prompt, List):\n",
    "            prompt = [prompt] * num_prompts\n",
    "        if not isinstance(negative_prompt, List):\n",
    "            negative_prompt = [negative_prompt] * num_prompts\n",
    "\n",
    "\n",
    "        # projection에서 나온 embedding\n",
    "        # clip과 shape가 같은듯\n",
    "        image_prompt_embeds, uncond_image_prompt_embeds = self.get_image_embeds(pil_image)\n",
    "        bs_embed, seq_len, _ = image_prompt_embeds.shape\n",
    "        image_prompt_embeds = image_prompt_embeds.repeat(1, num_samples, 1)\n",
    "        image_prompt_embeds = image_prompt_embeds.view(bs_embed * num_samples, seq_len, -1)\n",
    "\n",
    "        # prompt, do_classifier_free_guidance, negative_prompt,\n",
    "        # prompt_embeds=prompt_embeds, negative_prompt_embeds\n",
    "        # 넣어서 prompt_embeds를 얻는 _encode_prompt를 사용함\n",
    "        with torch.inference_mode():\n",
    "            # 그런데 prompt에 대한 promt_embeds, negative_prompt를 넣음\n",
    "            prompt_embeds = self.pipe._encode_prompt(\n",
    "                prompt, device=self.device, num_images_per_prompt=num_samples, do_classifier_free_guidance=True, negative_prompt=negative_prompt)\n",
    "\n",
    "            negative_prompt_embeds_, prompt_embeds_ = prompt_embeds.chunk(2)\n",
    "            prompt_embeds = torch.cat([prompt_embeds_, image_prompt_embeds], dim=1)\n",
    "            negative_prompt_embeds = torch.cat([negative_prompt_embeds_, uncond_image_prompt_embeds], dim=1)\n",
    "\n",
    "        generator = torch.Generator(self.device).manual_seed(seed) if seed is not None else None\n",
    "        # prompt embeds를 넣음\n",
    "        # 여기선 prompt가 없음\n",
    "        images = self.pipe(\n",
    "            prompt_embeds=prompt_embeds,\n",
    "            negative_prompt_embeds=negative_prompt_embeds,\n",
    "            guidance_scale=guidance_scale,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            generator=generator,\n",
    "            **kwargs,# 나머지 kwargs 다 들어갈 수 있게 # controlnet_conditioning_scale=0.7, image=depth_map\n",
    "        ).images\n",
    "\n",
    "        return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_path = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "image_encoder_path = \"models/image_encoder\"\n",
    "ip_ckpt = \"models/ip-adapter_sdxl_vit-h.bin\"\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "controlnet_path = \"diffusers/controlnet-depth-sdxl-1.0\"\n",
    "controlnet = ControlNetModel.from_pretrained(controlnet_path, variant=\"fp16\", use_safetensors=True, torch_dtype=torch.float16).to(device)\n",
    "pipe = StableDiffusionXLControlNetPipeline.from_pretrained(\n",
    "    base_model_path,\n",
    "    controlnet=controlnet,\n",
    "    use_safetensors=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    add_watermarker=False,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_model = IPAdapterXL(pipe, image_encoder_path, ip_ckpt, device)\n",
    "\n",
    "images = ip_model.generate(pil_image=image, image=depth_map, controlnet_conditioning_scale=0.7, num_samples=num_samples, num_inference_steps=30, seed=42)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hugging",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
