{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "face embedding + controlnet train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "\n",
    "from torch.nn import Linear, Conv2d, BatchNorm1d, BatchNorm2d, PReLU, Dropout, Sequential, Module\n",
    "from helpers import get_blocks, Flatten, bottleneck_IR, bottleneck_IR_SE, l2_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_tensor = torchvision.transforms.ToTensor()\n",
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, json_file, tokenizer, size=512, t_drop_rate=0.05, i_drop_rate=0.05, ti_drop_rate=0.05, image_root_path=\"\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.size = size\n",
    "        self.i_drop_rate = i_drop_rate\n",
    "        self.t_drop_rate = t_drop_rate\n",
    "        self.ti_drop_rate = ti_drop_rate\n",
    "        self.image_root_path = image_root_path\n",
    "\n",
    "    def fr_image_preprocess(self, images=raw_image):\n",
    "        # pil to tensor\n",
    "        raw_image = raw_image.resize((256, 256), Image.BILINEAR)\n",
    "        img = to_tensor(image)\n",
    "        img = img * 2 - 1\n",
    "        img = torch.unsqueeze(img, 0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        text = item[\"text\"]\n",
    "        image_file = item[\"image_file\"]\n",
    "\n",
    "        # read image\n",
    "        raw_image = Image.open(os.path.join(self.image_root_path, image_file))\n",
    "        image = self.transform(raw_image.convert(\"RGB\"))\n",
    "        clip_image = self.clip_image_processor(images=raw_image, return_tensors=\"pt\").pixel_values\n",
    "        fr_image = self.fr_image_preprocess(im)\n",
    "\n",
    "\n",
    "        # drop\n",
    "        drop_image_embed = 0\n",
    "        rand_num = random.random()\n",
    "        if rand_num < self.i_drop_rate:\n",
    "            drop_image_embed = 1\n",
    "        elif rand_num < (self.i_drop_rate + self.t_drop_rate):\n",
    "            text = \"\"\n",
    "        elif rand_num < (self.i_drop_rate + self.t_drop_rate + self.ti_drop_rate):\n",
    "            text = \"\"\n",
    "            drop_image_embed = 1\n",
    "        # get text and tokenize\n",
    "        text_input_ids = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.tokenizer.model_max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        ).input_ids\n",
    "\n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"text_input_ids\": text_input_ids,\n",
    "            \"clip_image\": clip_image,\n",
    "            \"drop_image_embed\": drop_image_embed\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Backbone(Module):\n",
    "    def __init__(self, input_size, num_layers, mode='ir', drop_ratio=0.4, affine=True):\n",
    "        super().__init__()\n",
    "        blocks = get_blocks(num_layers)\n",
    "\n",
    "        if mode == 'ir':\n",
    "            unit_module = bottleneck_IR\n",
    "        elif mode == 'ir_se':\n",
    "            unit_module = bottleneck_IR_SE\n",
    "\n",
    "        self.input_layer = Sequential(Conv2d(3, 64, (3, 3), 1, 1, bias=False),\n",
    "                                      BatchNorm2d(64),\n",
    "                                      PReLU(64))\n",
    "        self.output_layer = Sequential(BatchNorm2d(512),\n",
    "                                       Dropout(drop_ratio),\n",
    "                                       Flatten(),\n",
    "                                       Linear(512 * 7 * 7, 512),\n",
    "                                       BatchNorm1d(512, affine=affine))\n",
    "\n",
    "        self.body = nn.ModuleList([])\n",
    "        for block in blocks:\n",
    "            for bottleneck in block:\n",
    "                self.body.append(unit_module(bottleneck.in_channel,\n",
    "                                           bottleneck.depth,\n",
    "                                           bottleneck.stride))\n",
    "    def forward(self, x):\n",
    "        x = self.input_layer(x)\n",
    "\n",
    "        samples = ()\n",
    "        for i, body_block in enumerate(self.body):\n",
    "            x = body_block(x)\n",
    "            samples = samples + (x,)\n",
    "        x = self.output_layer(x)\n",
    "        return l2_norm(x), samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ie = Backbone(input_size=112, num_layers=50, drop_ratio=0.6, mode='ir_se')\n",
    "ie.eval()\n",
    "\n",
    "x = torch.randn(1, 3, 112, 112)\n",
    "\n",
    "out, feat = ie(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FRImageProjModel(torch.nn.Module):\n",
    "    \"\"\"Projection Model\"\"\"\n",
    "    def __init__(self, cross_attention_dim=1024, fr_embeddings_dim=512, fr_extra_context_tokens=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.cross_attention_dim = cross_attention_dim\n",
    "        self.fr_extra_context_tokens = fr_extra_context_tokens\n",
    "        \n",
    "        self.proj = torch.nn.Linear(fr_embeddings_dim, self.fr_extra_context_tokens * cross_attention_dim)\n",
    "        self.norm = torch.nn.LayerNorm(cross_attention_dim)\n",
    "        \n",
    "    def forward(self, fr_embeds):\n",
    "        \"\"\"\n",
    "        fr_embeds : [B, 512]\n",
    "        clip_extra_context_tokens : [B, fr_extra_context_tokens, cross_attention_dim] \n",
    "        \"\"\"\n",
    "        embeds = fr_embeds\n",
    "        clip_extra_context_tokens = self.proj(embeds).reshape(-1, self.fr_extra_context_tokens, self.cross_attention_dim)\n",
    "        clip_extra_context_tokens = self.norm(clip_extra_context_tokens)\n",
    "        return clip_extra_context_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_proj_model = FRImageProjModel(\n",
    "    cross_attention_dim=1024,\n",
    "    fr_embeddings_dim=512,\n",
    "    fr_extra_context_tokens=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IPAdapter(nn.Moduel):\n",
    "    \"\"\"IP-Adapter\"\"\"\n",
    "    def __init__(self, unet, image_proj_model, adapter_modules):\n",
    "        super().__init__()\n",
    "        self.unet = unet\n",
    "        self.image_proj_model = image_proj_model\n",
    "        self.adapter_modules = adapter_modules\n",
    "\n",
    "    def forward(self, noisy_latents, timesteps, encoder_hidden_states, image_embeds):\n",
    "        ip_tokens = self.image_proj_model(image_embeds)\n",
    "        encoder_hidden_states = torch.cat([encoder_hidden_states, ip_tokens], dim=1)\n",
    "        # Predict the noise residual and compute loss\n",
    "        noise_pred = self.unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
    "        return noise_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "diffusers.models.attention_processor.AttnProcessor2_0 is not a Module subclass",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/tyk/codeclone/ip-adapter/face_emb_train.ipynb Cell 9\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/tyk/codeclone/ip-adapter/face_emb_train.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m         weights \u001b[39m=\u001b[39m {\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/tyk/codeclone/ip-adapter/face_emb_train.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=56'>57</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mto_k_ip.weight\u001b[39m\u001b[39m\"\u001b[39m: unet_sd[layer_name \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.to_k.weight\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/tyk/codeclone/ip-adapter/face_emb_train.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=57'>58</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mto_v_ip.weight\u001b[39m\u001b[39m\"\u001b[39m: unet_sd[layer_name \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.to_v.weight\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/tyk/codeclone/ip-adapter/face_emb_train.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=58'>59</a>\u001b[0m         }\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/tyk/codeclone/ip-adapter/face_emb_train.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=59'>60</a>\u001b[0m         attn_procs[name] \u001b[39m=\u001b[39m IPAttnProcessor(hidden_size\u001b[39m=\u001b[39mhidden_size, cross_attention_dim\u001b[39m=\u001b[39mcross_attention_dim)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/home/tyk/codeclone/ip-adapter/face_emb_train.ipynb#X33sdnNjb2RlLXJlbW90ZQ%3D%3D?line=62'>63</a>\u001b[0m adapter_modules \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mModuleList(unet\u001b[39m.\u001b[39;49mattn_processors\u001b[39m.\u001b[39;49mvalues())\n",
      "File \u001b[0;32m~/anaconda3/envs/hugging/lib/python3.9/site-packages/torch/nn/modules/container.py:279\u001b[0m, in \u001b[0;36mModuleList.__init__\u001b[0;34m(self, modules)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m    278\u001b[0m \u001b[39mif\u001b[39;00m modules \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 279\u001b[0m     \u001b[39mself\u001b[39m \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m modules\n",
      "File \u001b[0;32m~/anaconda3/envs/hugging/lib/python3.9/site-packages/torch/nn/modules/container.py:320\u001b[0m, in \u001b[0;36mModuleList.__iadd__\u001b[0;34m(self, modules)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__iadd__\u001b[39m(\u001b[39mself\u001b[39m, modules: Iterable[Module]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m'\u001b[39m\u001b[39mModuleList\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 320\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mextend(modules)\n",
      "File \u001b[0;32m~/anaconda3/envs/hugging/lib/python3.9/site-packages/torch/nn/modules/container.py:402\u001b[0m, in \u001b[0;36mModuleList.extend\u001b[0;34m(self, modules)\u001b[0m\n\u001b[1;32m    400\u001b[0m offset \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m)\n\u001b[1;32m    401\u001b[0m \u001b[39mfor\u001b[39;00m i, module \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(modules):\n\u001b[0;32m--> 402\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_module(\u001b[39mstr\u001b[39;49m(offset \u001b[39m+\u001b[39;49m i), module)\n\u001b[1;32m    403\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/hugging/lib/python3.9/site-packages/torch/nn/modules/module.py:596\u001b[0m, in \u001b[0;36mModule.add_module\u001b[0;34m(self, name, module)\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Adds a child module to the current module.\u001b[39;00m\n\u001b[1;32m    587\u001b[0m \n\u001b[1;32m    588\u001b[0m \u001b[39mThe module can be accessed as an attribute using the given name.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[39m    module (Module): child module to be added to the module.\u001b[39;00m\n\u001b[1;32m    594\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    595\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(module, Module) \u001b[39mand\u001b[39;00m module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 596\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m is not a Module subclass\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    597\u001b[0m         torch\u001b[39m.\u001b[39mtypename(module)))\n\u001b[1;32m    598\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(name, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    599\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mmodule name should be a string. Got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    600\u001b[0m         torch\u001b[39m.\u001b[39mtypename(name)))\n",
      "\u001b[0;31mTypeError\u001b[0m: diffusers.models.attention_processor.AttnProcessor2_0 is not a Module subclass"
     ]
    }
   ],
   "source": [
    "from diffusers import AutoencoderKL, DDPMScheduler, UNet2DConditionModel\n",
    "from transformers import CLIPTextModel, CLIPTokenizer, CLIPVisionModelWithProjection\n",
    "\n",
    "import os\n",
    "import random\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import json\n",
    "import itertools\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "from transformers import CLIPImageProcessor\n",
    "\n",
    "from diffusers import AutoencoderKL, DDPMScheduler, UNet2DConditionModel\n",
    "from transformers import CLIPTextModel, CLIPTokenizer, CLIPVisionModelWithProjection\n",
    "\n",
    "\n",
    "def is_torch2_available():\n",
    "    return hasattr(F, \"scaled_dot_product_attention\")\n",
    "\n",
    "if is_torch2_available():\n",
    "    from attention_processor import IPAttnProcessor2_0 as IPAttnProcessor, AttnProcessor2_0 as AttnProcessor, IPFRAttnProcessor2_0 as IPFRAttnProcessor\n",
    "else:\n",
    "    from attention_processor import IPAttnProcessor, AttnProcessor, IPFRAttnProcessor\n",
    "\n",
    "unet = UNet2DConditionModel.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"unet\",\n",
    "                                            cache_dir='/home/tyk/hf_cache')\n",
    "unet.requires_grad_(False)\n",
    "# nn.ModuleList로 넣으려면 nn.Module의 subclass여야 한다.\n",
    "# 그래서 다시 구현한 것\n",
    "\n",
    "attn_procs = {}\n",
    "unet_sd = unet.state_dict()\n",
    "for name in unet.attn_processors.keys():\n",
    "    cross_attention_dim = None if name.endswith(\"attn1.processor\") else unet.config.cross_attention_dim\n",
    "    if name.startswith(\"mid_block\"):\n",
    "        hidden_size = unet.config.block_out_channels[-1]\n",
    "    elif name.startswith(\"up_blocks\"):\n",
    "        block_id = int(name[len(\"up_blocks.\")])\n",
    "        hidden_size = list(reversed(unet.config.block_out_channels))[block_id]\n",
    "    elif name.startswith(\"down_blocks\"):\n",
    "        block_id = int(name[len(\"down_blocks.\")])\n",
    "        hidden_size = unet.config.block_out_channels[block_id]\n",
    "\n",
    "    if cross_attention_dim is None:\n",
    "        attn_procs[name] = AttnProcessor()\n",
    "    else:\n",
    "        # 기존 weight로 로드 시켜줌\n",
    "        layer_name = name.split(\".processor\")[0]\n",
    "        weights = {\n",
    "            \"to_k_ip.weight\": unet_sd[layer_name + \".to_k.weight\"],\n",
    "            \"to_v_ip.weight\": unet_sd[layer_name + \".to_v.weight\"],\n",
    "        }\n",
    "        attn_procs[name] = IPFRAttnProcessor(hidden_size=hidden_size, cross_attention_dim=cross_attention_dim,\n",
    "                                             fr_embeds_dim=512)\n",
    "        attn_procs[name].load_state_dict()\n",
    "\n",
    "\n",
    "# adapter_modules = torch.nn.ModuleList(unet.attn_processors.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "import json\n",
    "import itertools\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "from transformers import CLIPImageProcessor\n",
    "\n",
    "from diffusers import AutoencoderKL, DDPMScheduler, UNet2DConditionModel\n",
    "from transformers import CLIPTextModel, CLIPTokenizer, CLIPVisionModelWithProjection\n",
    "\n",
    "\n",
    "def is_torch2_available():\n",
    "    return hasattr(F, \"scaled_dot_product_attention\")\n",
    "\n",
    "if is_torch2_available():\n",
    "    from attention_processor import IPAttnProcessor2_0 as IPAttnProcessor, AttnProcessor2_0 as AttnProcessor, IPFRAttnProcessor2_0 as IPFRAttnProcessor\n",
    "else:\n",
    "    from attention_processor import IPAttnProcessor, AttnProcessor, IPFRAttnProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args = parse_args()\n",
    "# logging_dir = Path(args.output_dir, args.logging_dir)\n",
    "\n",
    "# accelerator_project_config = ProjectConfiguration(project_dir=args.output_dir, logging_dir=logging_dir)\n",
    "\n",
    "# accelerator = Accelerator(    \n",
    "#     mixed_precision=args.mixed_precision,\n",
    "#     log_with=args.report_to,\n",
    "#     project_config=accelerator_project_config,\n",
    "# )\n",
    "\n",
    "# if accelerator.is_main_process:\n",
    "#     if args.output_dir is not None:\n",
    "#         os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "noise_scheduler = DDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"scheduler\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"tokenizer\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"text_encoder\")\n",
    "vae = AutoencoderKL.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"vae\")\n",
    "unet = UNet2DConditionModel.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"unet\")\n",
    "# image_encoder = CLIPVisionModelWithProjection.from_pretrained(args.image_encoder_path)\n",
    "# freeze parameters of models to save more memory\n",
    "unet.requires_grad_(False)\n",
    "vae.requires_grad_(False)\n",
    "text_encoder.requires_grad_(False)\n",
    "image_encoder.requires_grad_(False)\n",
    "\n",
    "#ip-adapter\n",
    "image_proj_model = FRImageProjModel(\n",
    "    cross_attention_dim=unet.config.cross_attention_dim,\n",
    "    clip_embeddings_dim=512,\n",
    "    clip_extra_context_tokens=4,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init adapter modules\n",
    "attn_procs = {}\n",
    "unet_sd = unet.state_dict()\n",
    "for name in unet.attn_processors.keys():\n",
    "    cross_attention_dim = None if name.endswith(\"attn1.processor\") else unet.config.cross_attention_dim\n",
    "    if name.startswith(\"mid_block\"):\n",
    "        hidden_size = unet.config.block_out_channels[-1]\n",
    "    elif name.startswith(\"up_blocks\"):\n",
    "        block_id = int(name[len(\"up_blocks.\")])\n",
    "        hidden_size = list(reversed(unet.config.block_out_channels))[block_id]\n",
    "    elif name.startswith(\"down_blocks\"):\n",
    "        block_id = int(name[len(\"down_blocks.\")])\n",
    "        hidden_size = unet.config.block_out_channels[block_id]\n",
    "        \n",
    "    if cross_attention_dim is None:\n",
    "        attn_procs[name] = AttnProcessor()\n",
    "    else:\n",
    "        layer_name = name.split(\".processor\")[0]\n",
    "        weights = {\n",
    "            \"to_k_ip.weight\": unet_sd[layer_name + \".to_k.weight\"],\n",
    "            \"to_v_ip.weight\": unet_sd[layer_name + \".to_v.weight\"],\n",
    "        }\n",
    "        attn_procs[name] = IPAttnProcessor(hidden_size=hidden_size, cross_attention_dim=cross_attention_dim)\n",
    "        attn_procs[name].load_state_dict(weights)\n",
    "unet.set_attn_processor(attn_procs)\n",
    "adapter_modules = torch.nn.ModuleList(unet.attn_processors.values())\n",
    "\n",
    "ip_adapter = IPAdapter(unet, image_proj_model, adapter_modules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows*cols\n",
    "\n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "    grid_w, grid_h = grid.size\n",
    "    \n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# infernce\n",
    "class IPAdapterPlusFR:\n",
    "    def __init__(self, sd_pipe, image_encoder_path, ip_ckpt, device, num_tokens=4):\n",
    "        self.device = device\n",
    "        self.image_encoder_path = image_encoder_path\n",
    "        self.ip_ckpt = ip_ckpt\n",
    "        self.num_tokens = num_tokens\n",
    "\n",
    "        self.pipe = sd_pipe.to(self.device)\n",
    "        self.set_ip_adapter()\n",
    "\n",
    "        self.image_encoder = Backbone(input_size=112, num_layers=50, drop_ratio=0.6, mode='ir_se')\n",
    "        self.image_encoder.load_state_dict(torch.load(\"/workspace/ddgm/functions/arcface/model_ir_se50.pth\"))\n",
    "        self.pool = nn.AdaptiveAvgPool2d((256, 256))\n",
    "        self.face_pool = torch.nn.AdaptiveAvgPool2d((112, 112))\n",
    "        self.facenet.eval()\n",
    "\n",
    "    def image_process(self, ):\n",
    "        \n",
    "    def set_ip_adapter(self):\n",
    "        unet = self.pipe.unet\n",
    "        attn_procs = {}\n",
    "        for name in unet.attn_processors.keys():\n",
    "\n",
    "\n",
    "        unet.set_attn_processor(attn_procs)\n",
    "        if hasattr(self.pipe, \"controlnet\"):\n",
    "            if isinstance(self.pipe.controlnet, MultiControlNetModel):\n",
    "                for controlnet in self.pipe.controlnet.nets:\n",
    "                    controlnet.set_attn_processor(CNAttnProcessor(num_tokens=self.num_tokens))\n",
    "            else:\n",
    "                self.pipe.controlnet.set_attn_processor(CNAttnProcessor(num_tokens=self.num_tokens))\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, noisy_latents, timesteps, encoder_hidden_states, image_embeds):\n",
    "        ip_tokens = self.image_proj_model(image_embeds)\n",
    "        encoder_hidden_states = torch.cat([encoder_hidden_states, ip_tokens], dim=1)\n",
    "        noise_pred = self.unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
    "        return noise_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionControlNetPipeline, DDIMScheduler, AutoencoderKL, ControlNetModel\n",
    "from PIL import Image\n",
    "\n",
    "controlnet_model_path = \"lllyasviel/control_v11p_sd15_openpose\"\n",
    "base_model_path = \"runwayml/stable-diffusion-v1-5\"\n",
    "vae_model_path = \"stabilityai/sd-vae-ft-mse\"\n",
    "image_encoder_path = \"models/image_encoder/\"\n",
    "ip_ckpt = \"models/ip-adapter_sd15.bin\"\n",
    "device = \"cuda\"\n",
    "\n",
    "\n",
    "noise_scheduler = DDIMScheduler(\n",
    "    num_train_timesteps=1000,\n",
    "    beta_start=0.00085,\n",
    "    beta_end=0.012,\n",
    "    beta_schedule=\"scaled_linear\",\n",
    "    clip_sample=False,\n",
    "    set_alpha_to_one=False,\n",
    "    steps_offset=1,\n",
    ")\n",
    "vae = AutoencoderKL.from_pretrained(vae_model_path).to(dtype=torch.float16, cache_dir='/home/tyk/hf_cache')\n",
    "\n",
    "\n",
    "controlnet = ControlNetModel.from_pretrained(controlnet_model_path, torch_dtype=torch.float16, cache_dir='/home/tyk/hf_cache')\n",
    "# load SD pipeline\n",
    "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
    "    base_model_path,\n",
    "    controlnet=controlnet,\n",
    "    torch_dtype=torch.float16,\n",
    "    scheduler=noise_scheduler,\n",
    "    vae=vae,\n",
    "    feature_extractor=None,\n",
    "    safety_checker=None,\n",
    "    cache_dir='/home/tyk/hf_cache'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(\"assets/images/girl_face.png\")\n",
    "image.resize((256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openpose_image = Image.open(\"assets/structure_controls/openpose.png\")\n",
    "openpose_image.resize((256, 384))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ip-adapter\n",
    "ip_model = IPAdapter(pipe, image_encoder_path, ip_ckpt, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate\n",
    "images = ip_model.generate(pil_image=image, image=openpose_image, width=512, height=768, num_samples=4, num_inference_steps=50, seed=42)\n",
    "grid = image_grid(images, 1, 4)\n",
    "grid"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hugging",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
