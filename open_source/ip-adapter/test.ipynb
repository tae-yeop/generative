{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from diffusers.pipelines.controlnet import MultiControlNetModel\n",
    "from transformers import CLIPVisionModelWithProjection, CLIPImageProcessor\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, StableDiffusionImg2ImgPipeline, StableDiffusionInpaintPipelineLegacy, DDIMScheduler, AutoencoderKL\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_path = \"runwayml/stable-diffusion-v1-5\"\n",
    "vae_model_path = \"stabilityai/sd-vae-ft-mse\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53e91bf37217429fa5132280b5692c3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/547 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92a902a6706144e286e869e266c2dc02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)ch_model.safetensors:   0%|          | 0.00/335M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cef22baa92864195b80272e188a3aeb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have disabled the safety checker for <class 'diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. Both the diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .\n"
     ]
    }
   ],
   "source": [
    "\n",
    "noise_scheduler = DDIMScheduler(\n",
    "    num_train_timesteps=1000,\n",
    "    beta_start=0.00085,\n",
    "    beta_end=0.012,\n",
    "    beta_schedule=\"scaled_linear\",\n",
    "    clip_sample=False,\n",
    "    set_alpha_to_one=False,\n",
    "    steps_offset=1,\n",
    ")\n",
    "vae = AutoencoderKL.from_pretrained(vae_model_path, cache_dir='/home/tyk/hf_cache').to(dtype=torch.float16)\n",
    "\n",
    "\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    base_model_path,\n",
    "    torch_dtype=torch.float16,\n",
    "    scheduler=noise_scheduler,\n",
    "    vae=vae,\n",
    "    feature_extractor=None,\n",
    "    safety_checker=None,\n",
    "    cache_dir='/home/tyk/hf_cache'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = pipe.unet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_procs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor 1\n",
      "1280\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn2.processor 1\n",
      "1280\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor 1\n",
      "1280\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn2.processor 1\n",
      "1280\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor 1\n",
      "1280\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn2.processor 1\n",
      "1280\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn1.processor 2\n",
      "640\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn2.processor 2\n",
      "640\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn1.processor 2\n",
      "640\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn2.processor 2\n",
      "640\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn1.processor 2\n",
      "640\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn2.processor 2\n",
      "640\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn1.processor 3\n",
      "320\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn2.processor 3\n",
      "320\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn1.processor 3\n",
      "320\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn2.processor 3\n",
      "320\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn1.processor 3\n",
      "320\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn2.processor 3\n",
      "320\n"
     ]
    }
   ],
   "source": [
    "# key와 value에 attention을 넣고 적용 # CA만 바꾸도록 \n",
    "for name in unet.attn_processors.keys():\n",
    "    # SA 제외하고\n",
    "    cross_attention_dim = None if name.endswith(\"attn1.processor\") else unet.config.cross_attention_dim\n",
    "    # print(cross_attention_dim)\n",
    "\n",
    "    if name.startswith(\"mid_block\"):\n",
    "        # 제일 안쪽\n",
    "        hidden_size = unet.config.block_out_channels[-1]\n",
    "        # print(hidden_size)\n",
    "    elif name.startswith(\"up_blocks\"):\n",
    "        block_id = int(name[len(\"up_blocks.\")])\n",
    "        # print(name, block_id)\n",
    "        hidden_size = list(reversed(unet.config.block_out_channels))[block_id]\n",
    "        # print(hidden_size)\n",
    "    elif name.startswith(\"down_blocks\"):\n",
    "        block_id = int(name[len(\"down_blocks.\")])\n",
    "        hidden_size = unet.config.block_out_channels[block_id]\n",
    "    # SA이라면\n",
    "    if cross_attention_dim is None:\n",
    "        attn_procs[name] = AttnProcessor()\n",
    "    else:\n",
    "        attn_procs[name] = IPAttnProcessor(hidden_size=hidden_size, cross_attention_dim=cross_attention_dim,\n",
    "                scale=1.0,num_tokens= self.num_tokens).to(self.device, dtype=torch.float16)\n",
    "\n",
    "unet.set_attn_processor(attn_procs)\n",
    "if hasattr(self.pipe, \"controlnet\"):\n",
    "    if isinstance(self.pipe.controlnet, MultiControlNetModel):\n",
    "        for controlnet in self.pipe.controlnet.nets:\n",
    "            controlnet.set_attn_processor(CNAttnProcessor(num_tokens=self.num_tokens))\n",
    "    else:\n",
    "        self.pipe.controlnet.set_attn_processor(CNAttnProcessor(num_tokens=self.num_tokens))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hugging",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
