{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json 파일만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dir_path = '/home/aiteam/tykim/generative_model/framework/diffusion/ControlNet/training/images'\n",
    "img_list = [f for f in os.listdir(dir_path) if f.endswith('.jpg')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_path = '/home/aiteam/tykim/generative_model/framework/diffusion/ControlNet/training/segm'\n",
    "seg_list = [f for f in os.listdir(seg_path) if f.endswith('.png')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seg에 존재하는 것만 뽑아내기\n",
    "new_img_list = [img for img in img_list if img.replace('.jpg', '_segm.png') in seg_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12701"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_img_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['WOMEN-Tees_Tanks-id_00005310-06_4_full.jpg',\n",
       " 'WOMEN-Leggings-id_00000286-04_1_front.jpg',\n",
       " 'WOMEN-Sweaters-id_00001635-06_7_additional.jpg',\n",
       " 'WOMEN-Graphic_Tees-id_00006415-01_4_full.jpg',\n",
       " 'WOMEN-Skirts-id_00006097-02_4_full.jpg',\n",
       " 'WOMEN-Blouses_Shirts-id_00007773-04_4_full.jpg',\n",
       " 'WOMEN-Tees_Tanks-id_00002144-05_7_additional.jpg',\n",
       " 'WOMEN-Tees_Tanks-id_00003343-12_4_full.jpg',\n",
       " 'WOMEN-Graphic_Tees-id_00001887-01_4_full.jpg',\n",
       " 'WOMEN-Rompers_Jumpsuits-id_00000883-01_7_additional.jpg']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_img_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/aiteam/tykim/generative_model/framework/diffusion/ControlNet/training/captions.json', 'r') as f:\n",
    "    prmps = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_data = []\n",
    "for img_path in new_img_list:\n",
    "    # print(img_path)\n",
    "    if prmps.get(img_path) is not None:\n",
    "        data = {\"source\": os.path.join('segm', img_path.replace('.jpg', '_segm.png')), \"target\" : os.path.join('images', img_path), \"prompt\": prmps[img_path]}\n",
    "    # print(data)\n",
    "    whole_data.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/home/aiteam/tykim/generative_model/framework/diffusion/ControlNet/fashion_mm.json', 'w') as f:\n",
    "#     json.dump(whole_data, f)\n",
    "with open('/home/aiteam/tykim/generative_model/framework/diffusion/ControlNet/fashion_mm2.json', 'w') as f:\n",
    "    for data in whole_data:\n",
    "        f.write(json.dumps(data) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('dsad')\n",
    "with open('/purestorage/project/tyk/0_experiments/0_smp/ControlNet/training/captions.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "root = '/purestorage/project/tyk/0_experiments/0_smp/ControlNet/training'\n",
    "# img_list = list(Path(root, 'images').rglob('*.jpg'))\n",
    "# print(img_list[:10])\n",
    "# seg_list = list(Path(root, 'seg').rglob('*.png'))\n",
    "# print(img_list[0].name)\n",
    "\n",
    "import os\n",
    "dir_path = '/purestorage/project/tyk/0_experiments/0_smp/ControlNet/training/images'\n",
    "img_list = [f for f in os.listdir(dir_path) if f.endswith('.jpg')]\n",
    "\n",
    "print(img_list[:10])\n",
    "dir_path = '/purestorage/project/tyk/0_experiments/0_smp/ControlNet/training/segm'\n",
    "seg_list = [f for f in os.listdir(dir_path) if f.endswith('.png')]\n",
    "print(seg_list[:10])\n",
    "\n",
    "print(len(img_list), len(seg_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.data = []\n",
    "        with open('./training/captions.json', 'rt') as f:\n",
    "            for line in f:\n",
    "                self.data.append(json.loads(line))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "\n",
    "        source_filename = item['source']\n",
    "        target_filename = item['target']\n",
    "        prompt = item['prompt']\n",
    "\n",
    "        source = cv2.imread('./training/fill50k/' + source_filename)\n",
    "        target = cv2.imread('./training/fill50k/' + target_filename)\n",
    "\n",
    "        # Do not forget that OpenCV read images in BGR order.\n",
    "        source = cv2.cvtColor(source, cv2.COLOR_BGR2RGB)\n",
    "        target = cv2.cvtColor(target, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Normalize source images to [0, 1].\n",
    "        source = source.astype(np.float32) / 255.0\n",
    "\n",
    "        # Normalize target images to [-1, 1].\n",
    "        target = (target.astype(np.float32) / 127.5) - 1.0\n",
    "\n",
    "        return dict(jpg=target, txt=prompt, hint=source)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tprint('dsad')\n",
    "\twith open('/purestorage/project/tyk/0_experiments/0_smp/ControlNet/training/captions.json', 'r') as f:\n",
    "\t\tdata = json.load(f)\n",
    "\troot = '/purestorage/project/tyk/0_experiments/0_smp/ControlNet/training'\n",
    "\t# img_list = list(Path(root, 'images').rglob('*.jpg'))\n",
    "\t# print(img_list[:10])\n",
    "\t# seg_list = list(Path(root, 'seg').rglob('*.png'))\n",
    "\t# print(img_list[0].name)\n",
    "\n",
    "\timport os\n",
    "\tdir_path = '/purestorage/project/tyk/0_experiments/0_smp/ControlNet/training/images'\n",
    "\timg_list = [f for f in os.listdir(dir_path) if f.endswith('.jpg')]\n",
    "\n",
    "\tprint(img_list[:10])\n",
    "\tdir_path = '/purestorage/project/tyk/0_experiments/0_smp/ControlNet/training/segm'\n",
    "\tseg_list = [f for f in os.listdir(dir_path) if f.endswith('.png')]\n",
    "\tprint(seg_list[:10])\n",
    "\n",
    "\tprint(len(img_list), len(seg_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img = Image.open('/home/aiteam/tykim/generative_model/framework/diffusion/ControlNet/training/segm/MEN-Denim-id_00000080-01_7_additional_segm.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000, 0.0039, 0.0196, 0.0431, 0.0510, 0.0549, 0.0588])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "\n",
    "torch.unique(tt(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('/home/aiteam/tykim/generative_model/framework/diffusion/ControlNet/training/segm/MEN-Denim-id_00000080-01_7_additional_segm.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_t = (torch.tensor(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.uint8"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_t.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "result type Float can't be cast to the desired output type Byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorchvision\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mvutils\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m vutils\u001b[39m.\u001b[39;49msave_image(torch\u001b[39m.\u001b[39;49mtensor(img), \u001b[39m'\u001b[39;49m\u001b[39m/home/aiteam/tykim/generative_model/framework/diffusion/ControlNet/seg.png\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/3dfm/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/3dfm/lib/python3.8/site-packages/torchvision/utils.py:158\u001b[0m, in \u001b[0;36msave_image\u001b[0;34m(tensor, fp, format, **kwargs)\u001b[0m\n\u001b[1;32m    156\u001b[0m grid \u001b[39m=\u001b[39m make_grid(tensor, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    157\u001b[0m \u001b[39m# Add 0.5 after unnormalizing to [0, 255] to round to nearest integer\u001b[39;00m\n\u001b[0;32m--> 158\u001b[0m ndarr \u001b[39m=\u001b[39m grid\u001b[39m.\u001b[39;49mmul(\u001b[39m255\u001b[39;49m)\u001b[39m.\u001b[39;49madd_(\u001b[39m0.5\u001b[39;49m)\u001b[39m.\u001b[39mclamp_(\u001b[39m0\u001b[39m, \u001b[39m255\u001b[39m)\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m, torch\u001b[39m.\u001b[39muint8)\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m    159\u001b[0m im \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(ndarr)\n\u001b[1;32m    160\u001b[0m im\u001b[39m.\u001b[39msave(fp, \u001b[39mformat\u001b[39m\u001b[39m=\u001b[39m\u001b[39mformat\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: result type Float can't be cast to the desired output type Byte"
     ]
    }
   ],
   "source": [
    "import torchvision.utils as vutils\n",
    "\n",
    "vutils.save_image(torch.tensor(img), '/home/aiteam/tykim/generative_model/framework/diffusion/ControlNet/seg.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data = []\n",
    "with open('./training/fashion_mm.json', 'rt') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "source = cv2.imread('./training/' + data[0]['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
       "       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
       "       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
       "       156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
       "       169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
       "       195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
       "       208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
       "       221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
       "       234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n",
       "       247, 248, 249, 250, 251, 252, 253, 254, 255], dtype=uint8)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([None], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.unique(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'images/WOMEN-Tees_Tanks-id_00005310-06_4_full.jpg'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]['source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "segm = Image.open('./training/' + data[0]['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'P'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segm.mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "segm_np = np.array(segm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1101, 750) uint8 [ 0 11 13 14 15 16 17 21 22]\n"
     ]
    }
   ],
   "source": [
    "print(segm_np.shape, segm_np.dtype, np.unique(segm_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "segm_cv = cv2.imread('./training/' + data[0]['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1101, 750, 3) uint8 [  0  16  50  78 130 139 144 155 165 174 179 180 185 205 210 222 238 245\n",
      " 250 255]\n"
     ]
    }
   ],
   "source": [
    "print(segm_cv.shape, segm_cv.dtype, np.unique(segm_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "segm_cv = cv2.imread('./training/' + data[0]['target'], cv2.IMREAD_GRAYSCALE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1101, 750) uint8 [  0  66  76 134 155 156 199 201 223]\n"
     ]
    }
   ],
   "source": [
    "print(segm_cv.shape, segm_cv.dtype, np.unique(segm_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = cv2.imread('./training/' + data[0]['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1101, 750, 3) uint8 [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255]\n"
     ]
    }
   ],
   "source": [
    "print(t.shape, t.dtype, np.unique(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = cv2.cvtColor(t, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite(\"/home/aiteam/tykim/generative_model/framework/diffusion/ControlNet/output.jpg\", t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DFDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.data = []\n",
    "        with open('./training/fashion_mm.json', 'rt') as f:\n",
    "            for line in f:\n",
    "                self.data.append(json.loads(line))\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "\n",
    "        source_filename = item['source']\n",
    "        target_filename = item['target']\n",
    "        prompt = item['prompt']\n",
    "\n",
    "        source = cv2.imread(os.path.join('./training',source_filename))\n",
    "        target = cv2.imread(os.path.join('./training',target_filename))\n",
    "\n",
    "        # Do not forget that OpenCV read images in BGR order.\n",
    "        source = cv2.cvtColor(source, cv2.COLOR_BGR2RGB)\n",
    "        target = cv2.cvtColor(target, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Normalize source images to [0, 1].\n",
    "        source = source.astype(np.float32) / 255.0\n",
    "\n",
    "        # Normalize target images to [-1, 1].\n",
    "        target = (target.astype(np.float32) / 127.5) - 1.0\n",
    "\n",
    "        return dict(jpg=target, txt=prompt, hint=source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DFDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = cv2.imread('./training' + dataset.data[0]['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./trainingsegm/WOMEN-Tees_Tanks-id_00005310-06_4_full_segm.png'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'./training' + dataset.data[0]['source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m source\u001b[39m.\u001b[39;49mshape\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "source.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = cv2.cvtColor(source, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1101, 750, 3)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]['jpg'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.cvtColor(source, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite('/home/aiteam/tykim/generative_model/framework/diffusion/ControlNet/output.jpg', dataset[0]['hint']*255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = cv2.imread('./training/' + dataset.data[0]['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = cv2.cvtColor(t, cv2.COLOR_BGR2RGB)\n",
    "cv2.imwrite(\"/home/aiteam/tykim/generative_model/framework/diffusion/ControlNet/output.jpg\", t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3dfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
