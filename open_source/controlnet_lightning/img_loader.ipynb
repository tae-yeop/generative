{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jpeg4py as jpeg\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "import pyspng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import DatasetFolder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "import cv2\n",
    "import jpeg4py as jpeg\n",
    "\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "def pil_loader(path):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        PIL Image\n",
    "    \"\"\"\n",
    "    with open(path, 'rb') as f:\n",
    "        img = Image.open(f)\n",
    "        return img.convert('RGB')\n",
    "    \n",
    "def opencv_loader(path):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        images(np.array [H, W, C])\n",
    "    \"\"\"\n",
    "    image = cv2.imread(path)\n",
    "    return image\n",
    "\n",
    "def jpeg4py_loader(path):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        images(np.array [H, W, C])\n",
    "    \"\"\"\n",
    "    image = jpeg.JPEG(path).decode()\n",
    "    return image\n",
    "\n",
    "def jp4pil_loader(path):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        images(np.array [H, W, C])\n",
    "    \"\"\"\n",
    "    image = jpeg4py_loader(path)\n",
    "    return Image.fromarray(image.astype('uint8'), 'RGB')\n",
    "\n",
    "# def tfms_deco(original_func, image):\n",
    "#     return original_func(image)\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    IMG_EXTENSIONS = ('.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif', '.tiff', '.webp')\n",
    "    \n",
    "    def __init__(self, root_path, transform=None, loader_type='pil'):\n",
    "        self.root_path = root_path\n",
    "        self.file_list = [f for f in os.listdir(root_path) if f.endswith('.jpg')][:1000]\n",
    "        self.loader = self._make_loader(loader_type)\n",
    "        self.loader_type = loader_type\n",
    "        if transform is not None:\n",
    "            self.transform = transform\n",
    "        else:\n",
    "            self._make_transforms()\n",
    "        \n",
    "    # def _setup_transform(self, transform):\n",
    "    #     \"\"\"\n",
    "    #     albumentation은 사용할 때 지정된 signature 그대로 사용해야 함\n",
    "    #     \"\"\"\n",
    "    #     if transform.__class__ == 'albumentations.core.composition.Compose':\n",
    "    #         def albm_tfms(image):\n",
    "    #             return transform(image=image)['image']\n",
    "    #         self.transform = albm_tfms\n",
    "    #     else:\n",
    "    #         self.transform = transform\n",
    "\n",
    "    def _make_transforms(self):\n",
    "        if self.loader_type == 'pil':\n",
    "            self.transform = transforms.Compose([transforms.ToTensor()])\n",
    "        elif self.loader_type == 'opencv':\n",
    "            def opencv_tfms(img):\n",
    "                return torch.from_numpy(img.transpose(2, 0, 1))\n",
    "            self.transform = opencv_tfms\n",
    "        elif self.loader_type == 'jpeg4py':\n",
    "            def jpeg4py_tfms(img):\n",
    "                return torch.from_numpy(img.transpose(2, 0, 1))\n",
    "            self.transform = jpeg4py_tfms\n",
    "        else:\n",
    "            self.transform = None\n",
    "\n",
    "            \n",
    "    def _make_loader(self, loader_type):\n",
    "        if loader_type == 'opencv':\n",
    "            return opencv_loader\n",
    "        elif loader_type == 'jpeg4py':\n",
    "            return jpeg4py_loader\n",
    "        elif loader_type =='jp4pil':\n",
    "            return jp4pil_loader\n",
    "        else:\n",
    "            return pil_loader\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path= self.file_list[idx]\n",
    "        # File Loading\n",
    "        img = self.loader(os.path.join(self.root_path, img_path))\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = '/home/aiteam/tykim/generative_model/framework/diffusion/ControlNet/training/images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_ds = CustomDataset(root_path, loader_type='pil')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MEN-Tees_Tanks-id_00003425-05_3_back.jpg',\n",
       " 'WOMEN-Tees_Tanks-id_00000871-02_2_side.jpg',\n",
       " 'WOMEN-Dresses-id_00002631-05_7_additional.jpg',\n",
       " 'WOMEN-Tees_Tanks-id_00005310-06_4_full.jpg',\n",
       " 'WOMEN-Leggings-id_00000286-04_1_front.jpg',\n",
       " 'WOMEN-Tees_Tanks-id_00002136-02_1_front.jpg',\n",
       " 'WOMEN-Blouses_Shirts-id_00006642-04_2_side.jpg',\n",
       " 'WOMEN-Sweaters-id_00001635-06_7_additional.jpg',\n",
       " 'WOMEN-Cardigans-id_00001436-02_7_additional.jpg',\n",
       " 'WOMEN-Graphic_Tees-id_00006415-01_4_full.jpg']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_ds.file_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1101, 750])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_ds[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(custom_ds, batch_size=16, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사이즈가 일관되지 않아서 Resize해야 함\n",
    "%%timeit -r 3 -n 3\n",
    "simple_load_times = []\n",
    "start_time = time.time()\n",
    "for image in dataloader:\n",
    "    image = image.cuda()\n",
    "    pass\n",
    "pil_time = time.time() - start_time\n",
    "simple_load_times.append(pil_time)\n",
    "print(str(simple_load_times) + ' sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1101, 750, 3)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "source = cv2.imread('/home/aiteam/tykim/generative_model/framework/diffusion/ControlNet/training/images/MEN-Denim-id_00000089-02_7_additional.jpg')\n",
    "source = cv2.cvtColor(source, cv2.COLOR_BGR2RGB)\n",
    "source = source.astype(np.float32) / 255.0\n",
    "source.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = source[None, ...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = rearrange(source, 'b h w c -> b c h w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m x\u001b[39m=\u001b[39mx\u001b[39m.\u001b[39;49mto(memory_format\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mcontiguous_format)\u001b[39m.\u001b[39mfloat()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "x=x.to(memory_format=torch.contiguous_format).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3dfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
